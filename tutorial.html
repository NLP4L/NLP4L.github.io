<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <link rel="stylesheet" href="github.css">
</head>
<body>
<h3 id="contents">Contents</h3>
<ul>
<li><a href="#gettingStarted">Let's Get Started!</a></li>
<li><a href="#install">Installing NLP4L</a></li>
<li><a href="#getCorpora">Obtaining Practice Corpus</a>
<ul>
<li><a href="#getCorpora_repl">NLP4L Interactive Shell</a></li>
<li><a href="#getCorpora_index">What is Index?</a></li>
<li><a href="#getCorpora_ldcc">Obtaining livedoor News Corpus and Creating Index</a></li>
<li><a href="#getCorpora_book">Creating Index with Data in the Companion CD-ROM that Accompanies the book &quot;言語研究のための統計入門&quot; (ISBN978-4-87424-498-2)</a></li>
<li><a href="#getCorpora_brown">Obtaining Brown Corpus and Creating an Index</a></li>
<li><a href="#getCorpora_reuters">Obtaining Reuters Corpus and Creating an Index</a></li>
<li><a href="#getCorpora_wiki">Obtaining Wikipedia Data and Creating an Index</a></li>
<li><a href="#getCorpora_schema">NLP4L Schema</a></li>
<li><a href="#getCorpora_csv">Importing a CSV File</a></li>
</ul></li>
<li><a href="#useNLP">Using as NLP Tool</a>
<ul>
<li><a href="#useNLP_wordcounts">Counting the Number of Words</a></li>
</ul></li>
<li><a href="#indexBrowser">Using Index Browser</a></li>
<li><a href="#dearSolrUsers">To Solr Users</a></li>
<li><a href="#dearESUsers">To Elasticsearch Users</a></li>
<li><a href="#useWithMahout">Working with Mahout</a></li>
<li><a href="#useWithSpark">Working with Spark</a></li>
<li><a href="#useLucene">Using Lucene</a></li>
<li><a href="#develop">Developing and Executing NLP4L Programs</a></li>
<li><a href="#tm">Attribution</a></li>
</ul>
<h1 id="gettingStarted">Let's Get Started!</h1>
<h1 id="install">Installing NLP4L</h1>
<h1 id="getCorpora">Obtaining Practice Corpus</h1>
<p>Before start analyzing your own text file using NLP4L, you are encouraged to use a practice corpus to check the action. Analyzing your own text file straight away may results in longer time to successfully complete the process or wondering about the ways to evaluate the analysis.</p>
<p>You will be able to actually try the followings and understand them easier if you first create an index using a practice corpus explained here.</p>
<p>Note that the corpora introduced here, except for the livedoor news corpus and wikipedia, are for research purpose only. Please be very careful using them.</p>
<h2 id="getCorpora_repl">NLP4L Interactive Shell</h2>
<p>NLP4L has an interactive shell that comes in handy for running commands and Scala codes. Start an interactive shell (REPL) as follows when NLP4L has been built.</p>
<pre class="shell"><code>$ ./target/pack/bin/nlp4l
Welcome to NLP4L!
Type in expressions to have them evaluated.
Type :help for more information
Type :? for information about NLP4L utilities
nlp4l&gt;</code></pre>
<h2 id="getCorpora_index">What is Index?</h2>
<p>NLP4L saves text files, which are put into the natural language process, in the inverted index for Lucene. An inverted index is a file structure that is organized so that you can use a word as the key to obtain a list of document numbers. We will refer to the inverted index simply as &quot;index&quot; in this document.</p>
<p>You can use an NLP4L function to create an index from a text file or otherwise make an existing index created by Apache Solr or Elasticsearch the target of NLP4L process. When you do the latter, however, pay good attention to the version of Lucene that Solr and Elasticsearch are using. Index created using the version that is too old may not sometimes be read by NLP4L Lucene library.</p>
<p>The followings will discuss how to obtain a practice corpus (text file) and create an index from scratch.</p>
<h2 id="getCorpora_ldcc">Obtaining livedoor News Corpus and Creating Index</h2>
<p>Execute the following to download and expand a livedoor news corpus from the RONDHUIT site.</p>
<pre class="shell"><code>$ mkdir -p ${nlp4l}/corpora/ldcc
$ cd ${nlp4l}/corpora/ldcc
$ wget http://www.rondhuit.com/ download /ldcc-20140209.tar.gz
$ tar xvzf ldcc-20140209.tar.gz</code></pre>
<p>Windows users: Please adjust to your own environment as appropriate to execute the scripts.</p>
<p>[Note] NLP4L interactive shell provides commands (Supports Unix-like operating systems only) to execute the above procedures.</p>
<pre class="shell"><code>nlp4l&gt; downloadLdcc
Successfully downloaded ldcc-20140209.tar.gz
Try to execute system command: tar xzf /Users/tomoko/repo/NLP4L/corpora/ldcc/ldcc-20140209.tar.gz -C /Users/tomoko/repo/NLP4L/corpora/ldcc
Success.</code></pre>
<p>Expanding livedoor news corpus will create subdirectories with the following category names directly under the text directory</p>
<pre class="shell"><code>$ ls -l text
total 16
-rw-r--r-- 1 koji staff 223 9 16 2012 CHANGES.txt
-rw-r--r-- 1 koji staff 2182 9 13 2012 README.txt
drwxr-xr-x 873 koji staff 29682 2 9 2014 dokujo-tsushin
drwxr-xr-x 873 koji staff 29682 2 9 2014 it-life-hack
drwxr-xr-x 867 koji staff 29478 2 9 2014 kaden-channel
drwxr-xr-x 514 koji staff 17476 2 9 2014 livedoor-homme
drwxr-xr-x 873 koji staff 29682 2 9 2014 movie-enter
drwxr-xr-x 845 koji staff 28730 2 9 2014 peachy
drwxr-xr-x 873 koji staff 29682 2 9 2014 smax
drwxr-xr-x 903 koji staff 30702 2 9 2014 sports-watch
drwxr-xr-x 773 koji staff 26282 2 9 2014 topic-news</code></pre>
<p>In addition, each subdirectory has files where one file contains one article. An article file looks like as follows.</p>
<pre class="shell"><code>$ head -n 5 text/sports-watch/sports-watch-6577722.txt
http://news.livedoor.com/article/detail/6577722/
2012-05-21T09:00:00+0900
渦中の香川真司にインタビュー、「ズバリ次のチーム、話を伺いたい」
20日放送、NHK「サンデースポーツ」では、山岸舞彩キャスターが日本代表・香川真司に行ったインタビューの模様を放送した。
</code></pre>
<p>The first line is the URL, the second line is the date, the third line is the title, and the fourth and the followings are the body of livedoor news article respectively.</p>
<p>Next, we will execute the examples/index_ldcc.scala program from the nlp4l command prompt to add livedoor news corpus to the Lucene index. To do so, start nlp4l as follows and use the load command to execute the examples/index_ldcc.scala program.</p>
<pre class="shell"><code>$ bin/nlp4l
nlp4l&gt; :load examples/index_ldcc.scala</code></pre>
<p>At the beginning of this program, Lucene index directory created in the manner described later is defined as follows.</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> index = <span class="st">&quot;/tmp/index-ldcc&quot;</span></code></pre>
<p>If this directory does not fit well (such as when you are using Windows), change it to point to another place and use the load command to execute the program again.</p>
<p>The Lucene index directory will look like as follows when this program is executed.</p>
<pre class="shell"><code>$ ls -l /tmp/index-ldcc
total 67432
-rw-r--r-- 1 koji wheel 16359884 2 24 13:40 _1.fdt
-rw-r--r-- 1 koji wheel 4963 2 24 13:40 _1.fdx
-rw-r--r-- 1 koji wheel 520 2 24 13:40 _1.fnm
-rw-r--r-- 1 koji wheel 7505 2 24 13:40 _1.nvd
-rw-r--r-- 1 koji wheel 147 2 24 13:40 _1.nvm
-rw-r--r-- 1 koji wheel 453 2 24 13:40 _1.si
-rw-r--r-- 1 koji wheel 11319391 2 24 13:40 _1.tvd
-rw-r--r-- 1 koji wheel 5636 2 24 13:40 _1.tvx
-rw-r--r-- 1 koji wheel 2169767 2 24 13:40 _1_Lucene50_0.doc
-rw-r--r-- 1 koji wheel 3322315 2 24 13:40 _1_Lucene50_0.pos
-rw-r--r-- 1 koji wheel 1272515 2 24 13:40 _1_Lucene50_0.tim
-rw-r--r-- 1 koji wheel 26263 2 24 13:40 _1_Lucene50_0.tip
-rw-r--r-- 1 koji wheel 136 2 24 13:40 segments_1
-rw-r--r-- 1 koji wheel 0 2 24 13:40 write.lock</code></pre>
<h2 id="getCorpora_book">Creating Index with Data in the Companion CD-ROM that Accompanies the book &quot;言語研究のための統計入門&quot; (ISBN978-4-87424-498-2)</h2>
<p>If you have the following book [1], you can use the data in CD-ROM that accompanies this book as your corpus. Otherwise, please proceed to the next section.</p>
<pre class="shell"><code>[1] 言語研究のための統計入門
石川慎一郎、前田忠彦、山崎誠 編
くろしお出版
ISBN978-4-87424-498-2</code></pre>
<p>Copy folders under &quot;INDIVIDUAL WRITERS&quot; (excluding &quot;INDIVIDUAL WRITERS&quot;) and folders under &quot;PLAIN&quot;(including &quot;PLAIN&quot;) in the data of CD-ROM that accompanies the book &quot;言語研究のための統計入門&quot; to corpora/CEEAUS. When copying is complete, check to see if it looks like the followings.</p>
<pre class="shell"><code># Creating a Directory
$ mkdir -p corpora/CEEAUS

# Copy the CD-ROM

# Confirm the Copied Contents
$ find corpora/CEEAUS -type d
corpora/CEEAUS
corpora/CEEAUS/CEECUS
corpora/CEEAUS/CEEJUS
corpora/CEEAUS/CEENAS
corpora/CEEAUS/CJEJUS
corpora/CEEAUS/PLAIN</code></pre>
<p>The CEEAUS corpus is characterized by its highly controlled writing condition. There are 2 articles with respective themes; one is about &quot;Part-time jobs of college students&quot; (file names with &quot;ptj&quot;) and the other is &quot;Complete nonsmoking in restaurants&quot; (file names with &quot;smk&quot;). Subdirectories under CEEAUS are divided as follows.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Subdirectories</th>
<th style="text-align: left;">Contents</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">CEEJUS</td>
<td style="text-align: left;">日本人大学生による英作文770本(770 English essays by Japanese college students)</td>
</tr>
<tr class="even">
<td style="text-align: center;">CEECUS</td>
<td style="text-align: left;">中国人大学生による英作文92本(92 English essays by Chinese college students)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">CEENAS</td>
<td style="text-align: left;">成人 English 母語話者による英作文92本(92 English essays by adult native speakers)</td>
</tr>
<tr class="even">
<td style="text-align: center;">CJEJUS</td>
<td style="text-align: left;">日本人大学生による日本語作文50本(50 Japanese essays by Japanese college students)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">PLAIN</td>
<td style="text-align: left;">上記すべてを含む(All of the above)</td>
</tr>
</tbody>
</table>
<p>Now we will create a Lucene index from the CEEAUS corpus. First, use corpora other than PLAIN to create a Lucene index.</p>
<pre class="shell"><code>$ bin/nlp4l
nlp4l&gt; :load examples/index_ceeaus.scala</code></pre>
<p>Next, use PLAIN to create a Lucene index.</p>
<pre class="shell"><code>$ bin/nlp4l
nlp4l&gt; :load examples/index_ceeaus_all.scala</code></pre>
<p>As you can see by looking at the beginning of each program, each Lucene indexes are created in /tmp/index-ceeaus and /tmp/index-ceeaus-all respectively. As in the previous example, you need to rewrite the lines and execute the program again if you want to create them in other directories.</p>
<h2 id="getCorpora_brown">Obtaining Brown Corpus and Creating an Index</h2>
<p>Download the Brown corpus to corpora/brown and expand it as follows.</p>
<pre class="shell"><code>$ mkdir ${nlp4l}/corpora/brown
$ cd ${nlp4l}/corpora/brown
$ wget https://ia600503.us.archive.org/21/items/BrownCorpus/brown.zip
$ unzip brown.zip</code></pre>
<p>Windows users: Please adjust to your own environment as appropriate to execute the scripts.</p>
<p>[Note] NLP4L interactive shell provides commands (Supports Unix-like operating systems only) to execute the above procedures.</p>
<pre class="shell"><code>nlp4l&gt; downloadBrown
Successfully downloaded brown.zip
Try to execute system command: unzip -o /Users/tomoko/repo/NLP4L/corpora/brown/brown.zip -d /Users/tomoko/repo/NLP4L/corpora/brown
Success.</code></pre>
<p>Now, create the Lucene index at the nlp4l prompt.</p>
<pre class="shell"><code>$ bin/nlp4l
nlp4l&gt; :load examples/index_brown.scala</code></pre>
<p>As you can see by looking at the beginning of each program, the Lucene index is created in /tmp/index-brown. As in the previous example, you need to rewrite the lines and execute the program again if you want to create it in other directories.</p>
<h2 id="getCorpora_reuters">Obtaining Reuters Corpus and Creating an Index</h2>
<p>You can obtain the Reuters corpus by <a href="http://trec.nist.gov/data/reuters/reuters.html">applying to</a> NIST (National Institute of Standards and Technology). Here we will use an archive that you can download from <a href="http://www.daviddlewis.com/resources/testcollections/rcv1/">Dr. David D. Lewis's site</a> introduced in the NIST site in order to show you how to create indexes for your reference.</p>
<p>Download the Reuters corpus to corpora/reuters and expand it as follows.</p>
<pre class="shell"><code>$ mkdir ${nlp4l}/corpora/reuters
$ cd ${nlp4l}/corpora/reuters
$ wget http://www.daviddlewis.com/resources/testcollections/reuters21578/reuters21578.tar.gz
$ tar xvzf reuters21578.tar.gz</code></pre>
<p>Windows users: Please adjust to your own environment as appropriate to execute the scripts.</p>
<p>[Note] NLP4L interactive shell provides commands (Supports Unix-like operating systems only) to execute the above procedures.</p>
<pre class="shell"><code>nlp4l&gt; downloadReuters
Successfully downloaded reuters21578.tar.gz
Try to execute system command: tar xzf /Users/tomoko/repo/NLP4L/corpora/reuters/reuters21578.tar.gz -C /Users/tomoko/repo/NLP4L/corpora/reuters
Success.</code></pre>
<p>Create the Lucene index at the nlp4l prompt.</p>
<pre class="shell"><code>$ bin/nlp4l
nlp4l&gt; :load examples/index_reuters.scala</code></pre>
<p>Looking at the program in the same manner as before, you can see that Lucene index is created in /tmp/index-reuters. As in the previous example, you need to rewrite the lines and execute the program again if you want to create it in other directories.</p>
<h2 id="getCorpora_wiki">Obtaining Wikipedia Data and Creating an Index</h2>
<p>Wikipedia data is one of the most highly favored corpora for NLP study. However, as Wikipedia articles are written by the rule unique to Wikipedia, you need to make an effort to run preprocess to extract only the text data before loading it to an Lucene index. This additional effort might be a hurdle that you have to overcome when it comes to using Wikipedia data.</p>
<p>Now, we will show how to use <a href="https://github.com/diegoceccarelli/json-wikipedia">json-wikipedia</a> to quickly load Wikipedia data into a Lucene index.</p>
<h3 id="downloading-and-building-json-wikipedia">Downloading and building json-wikipedia</h3>
<p>First, create a work directory called &quot;work&quot; <a href="https://github.com/diegoceccarelli/json-wikipedia">json-wikipedia</a> and download json-wikipedia in that directory.</p>
<pre class="shell"><code>$ mkdir work
$ cd work
$ wget https://github.com/diegoceccarelli/json-wikipedia/archive/master.zip
$ unzip master.zip</code></pre>
<p>Next, build json-wikipedia in the directory that is created when you unzip and expand the file.</p>
<pre class="shell"><code>$ cd json-wikipedia-master
$ mvn assembly:assembly</code></pre>
<p>The JAR file created in the target directory will be referenced from NLP4L when you add Wikipedia data, which is converted to JSON, to a Lucene index.</p>
<pre class="shell"><code>$ ls target
archive-tmp json-wikipedia-1.0.0.jar
classes maven-archiver
generated-sources surefire-reports
generated-test-sources test-classes
json-wikipedia-1.0.0-jar-with-dependencies.jar</code></pre>
<h3 id="downloading-wikipedia-data-and-converting-to-json">Downloading Wikipedia Data and Converting to JSON</h3>
<p>Go to the respective links of languages on the <a href="https://dumps.wikimedia.org/backup-index.html">Wikipedia download site</a> - jawiki for Japanese, enwiki for English and so forth. Then, download and expand a file labeled XXwiki-YYYYMMDD-pages-articles.xml.bz2 (where XX is language and YYYYMMDD is date).</p>
<pre class="shell"><code>$ wget https://dumps.wikimedia.org/jawiki/20150512/jawiki-20150512-pages-articles.xml.bz2
$ bunzip2 jawiki-20150512-pages-articles.xml.bz2</code></pre>
<p>Then, execute json-wikipedia as follows to convert it to the JSON format.</p>
<pre class="shell"><code>$ ./scripts/convert-xml-dump-to-json.sh en jawiki-20150512-pages-articles.xml /tmp/jawiki.json</code></pre>
<p>Specify language in the first argument. For now, json-wikipedia supports very limited number of languages, including English (en) and Italian (it), and does not support Japanese. The above example, therefore, specifies English (en) in the first argument (it seems to work without any problem). It takes nearly 30 minutes to convert Japanese wikipedia to the JSON format.</p>
<h3 id="creating-lucene-index">Creating Lucene Index</h3>
<p>Finally, we will add data in the JSON format from NLP4L to the Lucene index. Note that the json-wikipedia JAR file built from this procedure needs to be included in the NLP4L class path.</p>
<pre class="shell"><code>$ ./target/pack/bin/nlp4l -cp json-wikipedia-1.0.0-jar-with-dependencies.jar 
nlp4l&gt; </code></pre>
<p>Only you have to do now is to execute examples/index_jawiki.scala as follows. You, however, have to duplicate this sample program and write other programs for other languages as this is a program for Japanese Wikipedia. What requires the most special attention is the schema setting file that the program refer to. Examples/schema/jawiki.conf specifies JapaneseAnalyzer as it processes Japanese. You might want to use StandardAnalyzer for other languages including English where words are separated by spaces.</p>
<pre class="shell"><code>nlp4l&gt; :load examples/index_jawiki.scala</code></pre>
<p>The Lucene index will be created in about 30 minutes for the Japanese Wikipedia.</p>
<h2 id="getCorpora_schema">NLP4L Schema</h2>
<p>The Lucene index is basically schemaless but NLP4L can set up a schema. Among the practice corpora, now let's look at schemas that are defined by the livedoor news corpus (ldcc) , CEEAUS, or the Brown corpus (brown).</p>
<p>The table below lists field names that each corpus has (x specifies that the field is available in the corresponding corpus).</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Field Names</th>
<th style="text-align: center;">ldcc</th>
<th style="text-align: center;">CEEAUS</th>
<th style="text-align: center;">brown</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">file</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr class="even">
<td style="text-align: center;">type</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">x</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">cat</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr class="even">
<td style="text-align: center;">url</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">date</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">title</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">body</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">x</td>
</tr>
<tr class="even">
<td style="text-align: center;">body_en</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">x</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">body_ws</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">x</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">body_ja</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">x</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">body_pos</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">x</td>
</tr>
</tbody>
</table>
<p>In the title and the following fields, Lucene Analyzer divides texts by word when corpora are added. Lucene Analyzer is sophisticated, supporting not only division but also normalization including stop word extraction, stemming, and variable character conversion. We will discuss how it performs division/normalization later.</p>
<p>Words are not divided in other fields when they are added, and the character strings of corpus are added as they are. Especially, as the cat field has document categories, it can be used in tasks including the document classification.</p>
<h3 id="ldcc-schema">ldcc Schema</h3>
<p>Title holds the title of news article while body holds the body of article. Lucene's JapaneseAnalyzer divides words in the both fields.</p>
<h3 id="ceeaus-schema">CEEAUS Schema</h3>
<p>Type holds the CEEJUS/CEECUS/CEENAS/CJEJUS type while cat holds ptj (part-time jobs of college students) /smk (complete nonsmoking in restaurants) category. Though body_en and body_ws hold the same sentence, Lucene StandardAnalyzer is applied to body_en while Lucene WhitespaceAnalyzer is applied to body_ws. These fields are used according to their purposes; the hypothesis test below uses body_en while correlation analysis uses body_ws. Body_ja holds Japanese text from the CJEJUS subcorpus. Lucene JapaneseAnalyzer is applied to this field.</p>
<h3 id="brown-schema">Brown Schema</h3>
<p>Each article in the Brown corpus has the word class tags attached by words as follows.</p>
<pre class="shell"><code>The/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta&#39;s/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn &#39;&#39;/&#39;&#39; that/cs any/dti irregularities/nns took/vbd place/nn ./.</code></pre>
<p>The body_pos field holds this text as it is while the body field holds the same contents but has its word class tags removed.</p>
<h2 id="getCorpora_csv">Importing a CSV File</h2>
<p>We have been using practice corpus to create Lucene indexes. Now, let's look at how to import an original CSV file to a Lucene index.</p>
<p>As an example, let's assume that we have a following CSV file.</p>
<pre class="shell"><code>$ cat &lt;&lt; EOF &gt; /tmp/data.csv
1, NLP4L, &quot;NLP4L is a natural language processing tool for Apache Lucene written in Scala.&quot;
2, NLP4L, &quot;The main purpose of NLP4L is to use the NLP technology to improve Lucene users&#39; search experience.&quot;
3, LUCENE, &quot;Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java.&quot;
4, SOLR, &quot;Solr is highly reliable, scalable and fault tolerant, providing distributed indexing, replication and load-balanced querying, automated failover and recovery, centralized configuration and more.&quot;
5, SOLR, &quot;Solr powers the search and navigation features of many of the world&#39;s largest internet sites.&quot;
EOF</code></pre>
<p>Now, suppose our schema file is as follows.</p>
<pre class="shell"><code>$ cat &lt;&lt; EOF &gt; /tmp/schema.conf
schema {
 defAnalyzer {
   class : org.apache.lucene.analysis.standard.StandardAnalyzer
 }
 fields　= [
   {
     name : id
     indexed : true
     stored : true
   }
   {
     name : cat
     indexed : true
     stored : true
   }
   {
     name : body
     analyzer : {
       tokenizer {
         factory : standard
    }
       filters = [
         {
           factory : lowercase
         }
       ]
     }
     indexed : true
     stored : true
     termVector : true
     positions : true
     offsets : true
   }
 ]
}
EOF</code></pre>
<p>Then, run the command to import CSV file as follows.</p>
<pre class="shell"><code>$ java -cp &quot;target/pack/lib/*&quot; org.nlp4l.core.CSVImporter --index /tmp/index-tmp --schema /tmp/schema.conf --fields id,cat,body /tmp/data.csv</code></pre>
<h1 id="useNLP">Using as NLP Tool</h1>
<p>We will discuss how to use NLP4L as an NLP tool. Please prepare at hand a practice corpus registered in the Lucene index discussed above so you can use it anytime.</p>
<h2 id="useNLP_wordcounts">Counting the Number of Words</h2>
<p>Counting the number of words that appear in the corpus is one of NLP processing fundamentals. NLP4L registers corpus to a Lucene index before processing and is very good at counting the number of words as a search engine (Lucene) has something called an inverted index that uses words as keys.</p>
<p>Now we will discuss how to use a Reuters corpus to find out the frequency that words appear. As a starter, copy the following program, paste it to the nlp4l prompt and run it. Note that here we omitted the nlp4l prompt for a program that extends to more than one line so you can easily handle copy and paste.</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (1)</span>
<span class="kw">import</span> org.<span class="fu">nlp4l</span>.<span class="fu">core</span>.<span class="fu">_</span>
<span class="kw">import</span> org.<span class="fu">nlp4l</span>.<span class="fu">core</span>.<span class="fu">analysis</span>.<span class="fu">_</span>
<span class="kw">import</span> org.<span class="fu">nlp4l</span>.<span class="fu">stats</span>.<span class="fu">WordCounts</span>

<span class="co">// (2)</span>
<span class="kw">val</span> index = <span class="st">&quot;/tmp/index-reuters&quot;</span>

<span class="co">// (3)</span>
<span class="kw">val</span> reader = <span class="fu">RawReader</span>(index)</code></pre>
<ol type="1">
<li>imports necessary Scala program packages where the WordCounts object is used for the word frequency . (2) specifies the Lucene index directory of Reuters corpus while (3) specifies the Lucene index directory to be used for RawReader in order to obtain reader. RawReader, in NLP4L, is a comparatively low level Reader. There also is a high level Reader called IReader that manages schema. We, however, will specifically use RawReader in order to avoid the trouble of passing a schema.</li>
</ol>
<p>Using the obtained reader, we will count the word frequency in the following. As Lucene has independent inverted index in every field, you need to specify a field name to perform such processes as counting number of words. In the following, we will specify a body field that has the entire body of Reuters story.</p>
<h3 id="total-word-count-and-unique-word-count">Total Word Count and Unique Word Count</h3>
<p>We will first discuss the total word count. Lucene originally has a function to return the total word count of an unspecified field. You can, therefore, easily find out the number by using the sumTotalTermFreq() function that is the Scala wrapper of this function.</p>
<pre class="shell"><code>nlp4l&gt; val total = reader.sumTotalTermFreq(&quot;body&quot;)
total: Long = 1899819</code></pre>
<p>The next is a unique word count that is the number of word types. The unique word count equals to the size of inverted index as it has a structure that uses unique words as keys. Lucene, of course, can easily check it. Its Scala wrapper will be something like follows.</p>
<pre class="shell"><code>nlp4l&gt; val count = reader.field(&quot;body&quot;).get.terms.size
count: Int = 64625</code></pre>
<h3 id="counting-by-words">Counting by Words</h3>
<p>Next, we will be more specific and try counting by words. You can count by words using the count() function of WordCounts object. However, you need some preparation because the count() function takes more than one argument. Refer to the following program - you can copy and paste the program and run it at the nlp4l prompt.</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (4)</span>
<span class="kw">val</span> allDS = reader.<span class="fu">universalset</span>()

<span class="co">// (5)</span>
<span class="kw">val</span> analyzer = <span class="fu">Analyzer</span>(<span class="kw">new</span> org.<span class="fu">apache</span>.<span class="fu">lucene</span>.<span class="fu">analysis</span>.<span class="fu">standard</span>.<span class="fu">StandardAnalyzer</span>(<span class="kw">null</span>.<span class="fu">asInstanceOf</span>[org.<span class="fu">apache</span>.<span class="fu">lucene</span>.<span class="fu">analysis</span>.<span class="fu">util</span>.<span class="fu">CharArraySet</span>]))

<span class="co">// (6)</span>
<span class="kw">val</span> allMap = WordCounts.<span class="fu">count</span>(reader, <span class="st">&quot;body&quot;</span>, Set.<span class="fu">empty</span>, allDS, -<span class="dv">1</span>, analyzer)</code></pre>
<ol start="4" type="1">
<li>obtains a target Lucene document number that is used by the count() function. We use the universalset() function, which obtains the total set of a document, as we target the all documents here. (5) then specifies StandardAnalyzer, a standard Analyzer for Lucene, to create the Scala Analyzer. The null, which is specified as an argument for StandardAnalyzer, specifies that you do not use any stop words (the default stop words for StandardAnalyzer will be used when null is not specified). The count() function in (6) calculates word frequency for every word. Set.empty that is specified as an argument specifies that &quot;the all words&quot; will be the target of count. &quot;-1&quot; specifies the number of words with most frequency from the top to &quot;-1&quot;. Specifying -1 means that the target will be the all words.</li>
</ol>
<p>Now the result is displayed but is hard to read in the way it is displayed now. We, therefore, limit the number of data that is displayed. For example, using Scala collection function to prepare the following will enable you to display 10 words that start with &quot;g&quot; and their frequency.</p>
<pre class="shell"><code>nlp4l&gt; allMap.filter(_._1.startsWith(&quot;g&quot;)).take(10).foreach(println(_))
(generalize,1)
(gress,1)
(germans,18)
(goiporia,2)
(garcin,2)
(granma,7)
(gorbachev&#39;s,10)
(gamble,9)
(grains,110)
(gienow,1)</code></pre>
<p>Add the all numbers resulting from allMap - the following program basically performs the same function as the totalCount() function.</p>
<pre class="shell"><code>nlp4l&gt; allMap.values.sum
res2: Long = 1899819</code></pre>
<p>This certainly matches the total word count that we first found out. Also, the size of allMap should match the unique word count. Let's find out.</p>
<pre class="shell"><code>nlp4l&gt; allMap.size
res3: Int = 64625</code></pre>
<p>Here we have another match.</p>
<p>The above count() passed Set.empty to target the all words in order to count the advent of frequency. You can also pass a specific word set instead of Set.empty to count only this word set - in NLP, you often want to count only the specific words. Let's find out.</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> whWords = Set(<span class="st">&quot;when&quot;</span>, <span class="st">&quot;where&quot;</span>, <span class="st">&quot;who&quot;</span>, <span class="st">&quot;what&quot;</span>, <span class="st">&quot;which&quot;</span>, <span class="st">&quot;why&quot;</span>)
<span class="kw">val</span> whMap = WordCounts.<span class="fu">count</span>(reader, <span class="st">&quot;body&quot;</span>, whWords, allDS, -<span class="dv">1</span>, analyzer)
whMap.<span class="fu">foreach</span>(<span class="fu">println</span>(_))</code></pre>
<p>The result will be as follows.</p>
<pre class="shell"><code>nlp4l&gt; whMap.foreach(println(_))
(why,125)
(what,850)
(who,1618)
(which,7556)
(where,507)
(when,1986)</code></pre>
<h3 id="counting-by-category">Counting by Category</h3>
<p>Next, let's look at how to find out the word frequency by category. For example, document classification, one of an NLP tasks, sometimes uses word frequency by category as its learning data in order to classify by category. This is the technique that you can use in such cases.</p>
<p>Here we use a sample Reuters corpus but will try the places field instead of category. To do so, we first find the words in the places field as follows.</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (7)</span>
reader.<span class="fu">terms</span>(<span class="st">&quot;places&quot;</span>).<span class="fu">get</span>.<span class="fu">map</span>(_.<span class="fu">text</span>)

<span class="co">// (8) When you want to fomat before displaying.</span>
reader.<span class="fu">terms</span>(<span class="st">&quot;places&quot;</span>).<span class="fu">get</span>.<span class="fu">map</span>(_.<span class="fu">text</span>).<span class="fu">foreach</span>(<span class="fu">println</span>(_))</code></pre>
<p>Running (7) or (8) will display the list of all words registered in the places field. Let's focus on usa and japan here. Run the program as shown in (9) to obtain respective document subsets.</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (9)</span>
<span class="kw">val</span> usDS = reader.<span class="fu">subset</span>(<span class="fu">TermFilter</span>(<span class="st">&quot;places&quot;</span>, <span class="st">&quot;usa&quot;</span>))
<span class="kw">val</span> jpDS = reader.<span class="fu">subset</span>(<span class="fu">TermFilter</span>(<span class="st">&quot;places&quot;</span>, <span class="st">&quot;japan&quot;</span>))</code></pre>
<p>Finally, pass the respective subsets obtained in (9) to the count() function to obtain the usa count and the japan count. In (10), however, we will quickly obtain the counts for two words: war and peace.</p>
<pre class="shell"><code>// (10)
nlp4l&gt; WordCounts.count(reader, &quot;body&quot;, Set(&quot;war&quot;, &quot;peace&quot;), usDS, -1, analyzer)
res22: Map[String,Long] = Map(war -&gt; 199, peace -&gt; 14)

nlp4l&gt; WordCounts.count(reader, &quot;body&quot;, Set(&quot;war&quot;, &quot;peace&quot;), jpDS, -1, analyzer)
res23: Map[String,Long] = Map(war -&gt; 75, peace -&gt; 2)</code></pre>
<p>Now we obtained counts for 2 words, war and peace, with place one for usa and the other for japan. Good! Or is it?</p>
<p>The fact is that the places field may have more than one name of place. In sum, an article of usa and that of japan may have some redundancy. Let's find out. As usDS and jpDS are Set collection objects of Scala, you can use the &amp; operator (function) to easily obtain product sets for the both.</p>
<pre class="shell"><code>nlp4l&gt; (usDS &amp; jpDS).size
res24: Int = 452</code></pre>
<p>Here we use size to obtain the size of product set. Now we can see there is a redundancy. In this case, you can use &amp;~ of Scala Set operator (function) as specified in (11) and (12) to get difference of sets to obtain word frequency for the portion that has no redundancy. Note that toSet is used here to do a conversion to Set because SortedSet does not have &amp;~ operator (function).</p>
<pre class="shell"><code>nlp4l&gt; // (11) Articles where places has a value usa but does not have a value japan will be the target.
nlp4l&gt; WordCounts.count(reader, &quot;body&quot;, Set(&quot;war&quot;, &quot;peace&quot;), usDS.toSet &amp;~ jpDS.toSet, -1, analyzer)
res25: Map[String,Long] = Map(war -&gt; 140, peace -&gt; 13)

nlp4l&gt; // (12) Articles where places has a value japan but does not have a value usa will be the target. 
nlp4l&gt; WordCounts.count(reader, &quot;body&quot;, Set(&quot;war&quot;, &quot;peace&quot;), jpDS.toSet &amp;~ usDS.toSet, -1, analyzer)
res26: Map[String,Long] = Map(war -&gt; 16, peace -&gt; 1)</code></pre>
<h3 id="visualizing-word-counts-experimental">Visualizing Word Counts (experimental)</h3>
<p>We have been counting words from several standpoints. Now, let's visualize word count data. Applications, such as Excel, enable visualization but we will use NLP4L here as it provides easy chart display tool on a trial basis. The chart display tool right now is experimental. Its functions and usage may change in the future to improve the feature.</p>
<p>Rerun the result of above word count and assign it in a variable.</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> usMap = WordCounts.<span class="fu">count</span>(reader, <span class="st">&quot;body&quot;</span>, Set(<span class="st">&quot;war&quot;</span>, <span class="st">&quot;peace&quot;</span>), usDS, -<span class="dv">1</span>, analyzer)
<span class="kw">val</span> jpMap = WordCounts.<span class="fu">count</span>(reader, <span class="st">&quot;body&quot;</span>, Set(<span class="st">&quot;war&quot;</span>, <span class="st">&quot;peace&quot;</span>), jpDS, -<span class="dv">1</span>, analyzer)</code></pre>
<p>Next, import a package for displaying charts (11) and obtain a presentation for chart display (12). Here, a data model obtained above is being passed to a presentation. Labels for explanatory notes are required as well when you need to pass a data model.</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (11)</span>
<span class="kw">import</span> org.<span class="fu">nlp4l</span>.<span class="fu">gui</span>.<span class="fu">_</span>

<span class="co">// (12)</span>
<span class="kw">val</span> presentation = <span class="fu">BarChart</span>(List((<span class="st">&quot;US&quot;</span>,usMap), (<span class="st">&quot;Japan&quot;</span>,jpMap)))

<span class="co">// (13)</span>
<span class="kw">val</span> server = <span class="kw">new</span> <span class="fu">SimpleHttpServer</span>(presentation)
server.<span class="fu">service</span></code></pre>
<p>As a Web server provides a chart display, you need to create and start a simple Web server using a presentation as an argument as specified in (13). The following message is displayed on the console when it boots up.</p>
<pre class="shell"><code>nlp4l&gt; server.service
WARNING: This function is experimental and might change in incompatible ways in the future release.

To see the chart, access this Uniform Resource Locators -&gt; http://localhost:6574/chart
To shutdown the server, access this Uniform Resource Locators -&gt; http://localhost:6574/shutdown</code></pre>
<p>Accessing the displayed <a href="http://localhost:6574/chart">URL</a> from a modern Web browser will display a bar chart. Access <a href="http://localhost:6574/shutdown" class="uri">http://localhost:6574/shutdown</a> to shut down the simple Web server.</p>
<figure>
<img src="barchart_wc.png" alt="Word frequency by category" /><figcaption>Word frequency by category</figcaption>
</figure>
<p>The codes for displaying the above chart is put together into one script as examples/chart_experimental.scala.</p>
<h1 id="indexBrowser">Using Index Browser</h1>
<h1 id="dearSolrUsers">To Solr Users</h1>
<h1 id="dearESUsers">To Elasticsearch Users</h1>
<h1 id="useWithMahout">Working with Mahout</h1>
<h1 id="useWithSpark">Working with Spark</h1>
<h1 id="useLucene">Using Lucene</h1>
<h1 id="develop">Developing and Executing NLP4L Programs</h1>
<h1 id="tm">Attribution</h1>
</body>
</html>
