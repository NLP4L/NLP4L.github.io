<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <link rel="stylesheet" href="github.css">
</head>
<body>
<h3 id="目次">目次</h3>
<ul>
<li><a href="#gettingStarted">はじめよう！</a></li>
<li><a href="#install">インストール</a></li>
<li><a href="#getCorpora">練習用コーパスの入手</a></li>
<li><a href="#useNLP">NLPツールとしての利用</a></li>
<li><a href="#indexBrowser">インデックスブラウザを使う</a></li>
<li><a href="#dearSolrUsers">Solrユーザの皆様</a></li>
<li><a href="#dearESUsers">Elasticsearchユーザの皆様</a></li>
<li><a href="#useWithMahout">Mahoutと連携する</a></li>
<li><a href="#useWithSpark">Sparkと連携する</a></li>
<li><a href="#useLucene">Luceneを使う</a></li>
<li><a href="#develop">NLP4Lプログラムを開発して実行する</a></li>
<li><a href="#tm">帰属</a></li>
</ul>
<h1 id="gettingStarted">はじめよう！</h1>
<h1 id="install">インストール</h1>
<h2 id="インストール後のディレクトリ構造">インストール後のディレクトリ構造</h2>
<p>インストール後のディレクトリ構造は以下のようになっています。${nlp4l} は、NLP4L のインストールディレクトリを指しています。</p>
<pre class="shell"><code>${nlp4l}/
    bin/
    docs/
    examples/
    lib/</code></pre>
<h1 id="getCorpora">練習用コーパスの入手</h1>
<p>NLP4Lを使って自分自身のテキストファイルの分析を始める前に、練習用コーパスを使って動作確認することをお勧めします。いきなり独自のテキストファイルを分析すると、うまく動作させるのに時間がかかったり、分析結果をどのように評価していいか悩んでしまうことがあるかもしれません。</p>
<p>ここで説明する練習用コーパスを使ってインデックスを作成しておくと、これ以降に書かれている解説も実際に試すことができるので理解も容易になるでしょう。</p>
<p>なおここで紹介するコーパスは、livedoorニュースコーパスとWikipediaを除き、研究目的以外での利用が禁止されています。使用に際しては十分ご注意ください。</p>
<h2 id="nlp4lの対話型シェル">NLP4Lの対話型シェル</h2>
<h2 id="インデックスとは">インデックスとは？</h2>
<p>NLP4Lでは自然言語処理を行うテキストファイルをLuceneの転置インデックスに保存します。転置インデックスは単語をキーにしてその単語を含むドキュメント番号のリストを得られるように整理されたファイル構造です。転置インデックスを本書では単にインデックスと呼ぶことにします。</p>
<p>インデックスはNLP4Lの機能を使ってテキストファイルから新規に作成することもできますし、Apache Solr や Elasticsearch を使って作られた既存のインデックスをNLP4Lの処理対象とすることもできます。ただしその場合は Solr や Elasticsearch が使っている Lucene のバージョンに注意しましょう。あまりに古いバージョンで作成されたインデックスはNLP4LのLuceneライブラリで読めない可能性もあります。</p>
<p>以降では練習用のコーパス（テキストファイル）を入手して新規にインデックスを作る方法を説明します。</p>
<h2 id="livedoorニュースコーパスの入手とインデックスの作成">livedoorニュースコーパスの入手とインデックスの作成</h2>
<p>次のように実行してロンウイットのサイトから livedoorニュースコーパスをダウンロードして展開します。</p>
<pre class="shell"><code>$ mkdir -p ${nlp4l}/corpora/ldcc
$ cd ${nlp4l}/corpora/ldcc
$ wget http://www.rondhuit.com/download/ldcc-20140209.tar.gz
$ tar xvzf ldcc-20140209.tar.gz</code></pre>
<p>Windows ユーザーの方は、適宜読み替えて実行してください。</p>
<p>[Note] NLP4L の対話型シェルには、上記手順を実行するコマンド(Unix系OSのみ対応)が用意されています。</p>
<pre class="shell"><code>nlp4l&gt; downloadLdcc
Successfully downloaded ldcc-20140209.tar.gz
Try to execute system command: tar xzf /Users/tomoko/repo/NLP4L/corpora/ldcc/ldcc-20140209.tar.gz -C /Users/tomoko/repo/NLP4L/corpora/ldcc
Success.</code></pre>
<p>livedoorニュースコーパスは展開するとtextディレクトリの直下に以下のようなカテゴリ名のついたサブディレクトリを持ちます。</p>
<pre class="shell"><code>$ ls -l text
total 16
-rw-r--r--    1 koji  staff    223  9 16  2012 CHANGES.txt
-rw-r--r--    1 koji  staff   2182  9 13  2012 README.txt
drwxr-xr-x  873 koji  staff  29682  2  9  2014 dokujo-tsushin
drwxr-xr-x  873 koji  staff  29682  2  9  2014 it-life-hack
drwxr-xr-x  867 koji  staff  29478  2  9  2014 kaden-channel
drwxr-xr-x  514 koji  staff  17476  2  9  2014 livedoor-homme
drwxr-xr-x  873 koji  staff  29682  2  9  2014 movie-enter
drwxr-xr-x  845 koji  staff  28730  2  9  2014 peachy
drwxr-xr-x  873 koji  staff  29682  2  9  2014 smax
drwxr-xr-x  903 koji  staff  30702  2  9  2014 sports-watch
drwxr-xr-x  773 koji  staff  26282  2  9  2014 topic-news</code></pre>
<p>さらにそれぞれのサブディレクトリの下に1記事1ファイルに分かれたファイルを持っています。1つの記事ファイルは次のようになっています。</p>
<pre class="shell"><code>$ head -n 5 text/sports-watch/sports-watch-6577722.txt
http://news.livedoor.com/article/detail/6577722/
2012-05-21T09:00:00+0900
渦中の香川真司にインタビュー、「ズバリ次のチーム、話を伺いたい」
20日放送、NHK「サンデースポーツ」では、山岸舞彩キャスターが日本代表・香川真司に行ったインタビューの模様を放送した。
</code></pre>
<p>1行目がlivedoorニュース記事URL、2行目が記事の日付、3行目が記事のタイトル、4行目以降が記事本文です。</p>
<p>では次に、nlp4l コマンドプロンプトから examples/index_ldcc.scala プログラムを実行してlivedoorニュースコーパスをLuceneインデックスに登録します。それには次のようにnlp4lを起動してloadコマンドで examples/index_ldcc.scala プログラムを実行すればOKです。</p>
<pre class="shell"><code>$ bin/nlp4l
nlp4l&gt; :load examples/index_ldcc.scala</code></pre>
<p>なお、このプログラムでは冒頭で、次のように作成するLuceneインデックスのディレクトリを次のように定義しています。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> index = <span class="st">&quot;/tmp/index-ldcc&quot;</span></code></pre>
<p>このディレクトリで都合が悪い場合（Windowsを使っているときなど）は別の場所を指すように変更してからloadコマンドで再度プログラムを実行してください。</p>
<p>このプログラムの実行後、Luceneインデックスのディレクトリは次のようになっています。</p>
<pre class="shell"><code>$ ls -l /tmp/index-ldcc
total 67432
-rw-r--r--  1 koji  wheel  16359884  2 24 13:40 _1.fdt
-rw-r--r--  1 koji  wheel      4963  2 24 13:40 _1.fdx
-rw-r--r--  1 koji  wheel       520  2 24 13:40 _1.fnm
-rw-r--r--  1 koji  wheel      7505  2 24 13:40 _1.nvd
-rw-r--r--  1 koji  wheel       147  2 24 13:40 _1.nvm
-rw-r--r--  1 koji  wheel       453  2 24 13:40 _1.si
-rw-r--r--  1 koji  wheel  11319391  2 24 13:40 _1.tvd
-rw-r--r--  1 koji  wheel      5636  2 24 13:40 _1.tvx
-rw-r--r--  1 koji  wheel   2169767  2 24 13:40 _1_Lucene50_0.doc
-rw-r--r--  1 koji  wheel   3322315  2 24 13:40 _1_Lucene50_0.pos
-rw-r--r--  1 koji  wheel   1272515  2 24 13:40 _1_Lucene50_0.tim
-rw-r--r--  1 koji  wheel     26263  2 24 13:40 _1_Lucene50_0.tip
-rw-r--r--  1 koji  wheel       136  2 24 13:40 segments_1
-rw-r--r--  1 koji  wheel         0  2 24 13:40 write.lock</code></pre>
<h2 id="書籍言語研究のための統計入門付属cd-romデータを使ったインデックスの作成">書籍「言語研究のための統計入門」付属CD-ROMデータを使ったインデックスの作成</h2>
<p>以下の書籍[1]をお持ちの方は、付属CD-ROMのデータをコーパスとして使えます。この書籍をお持ちでない方は次の節へお進みください。</p>
<pre class="shell"><code>[1] 言語研究のための統計入門
石川慎一郎、前田忠彦、山崎誠 編
くろしお出版
ISBN978-4-87424-498-2</code></pre>
<p>corpora/CEEAUS 以下に「言語研究のための統計入門」付属CD-ROMのデータの&quot;INDIVIDUAL WRITERS&quot;以下のフォルダー（&quot;INDIVIDUAL WRITERS&quot;は含まない）を、また、&quot;PLAIN&quot;以下のフォルダー（&quot;PLAIN&quot;を含む）をコピーしてください。コピー後、次のようになっていることを確認してください。</p>
<pre class="shell"><code># ディレクトリの作成
$ mkdir -p corpora/CEEAUS

# ここで CD-ROM をコピー

# コピー内容の確認
$ find corpora/CEEAUS -type d
corpora/CEEAUS
corpora/CEEAUS/CEECUS
corpora/CEEAUS/CEEJUS
corpora/CEEAUS/CEENAS
corpora/CEEAUS/CJEJUS
corpora/CEEAUS/PLAIN</code></pre>
<p>CEEAUSコーパスは執筆条件が高度に統制されたコーパスとなっているのが特徴です。記事は2種類あり、1つは「大学生のアルバイト」（ファイル名にptjがつくもの）、もう一つは「レストラン全面禁煙」（ファイル名にsmkがつくもの）というテーマについて書かれたものとなっています。CEEAUS以下のサブディレクトリは次のように分かれています。</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">サブディレクトリ</th>
<th style="text-align: left;">内容</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">CEEJUS</td>
<td style="text-align: left;">日本人大学生による英作文770本</td>
</tr>
<tr class="even">
<td style="text-align: center;">CEECUS</td>
<td style="text-align: left;">中国人大学生による英作文92本</td>
</tr>
<tr class="odd">
<td style="text-align: center;">CEENAS</td>
<td style="text-align: left;">成人英語母語話者による英作文92本</td>
</tr>
<tr class="even">
<td style="text-align: center;">CJEJUS</td>
<td style="text-align: left;">日本人大学生による日本語作文50本</td>
</tr>
<tr class="odd">
<td style="text-align: center;">PLAIN</td>
<td style="text-align: left;">上記すべてを含む</td>
</tr>
</tbody>
</table>
<p>ではCEEAUSコーパスからLuceneインデックスを作成します。最初に上記一覧表のPLAIN以外のコーパスからLuceneインデックスを作成します。</p>
<pre class="shell"><code>$ bin/nlp4l
nlp4l&gt; :load examples/index_ceeaus.scala</code></pre>
<p>次にPLAINからLuceneインデックスを作成します。</p>
<pre class="shell"><code>$ bin/nlp4l
nlp4l&gt; :load examples/index_ceeaus_all.scala</code></pre>
<p>それぞれのプログラムの冒頭を見ていただければわかりますが、Luceneインデックスはそれぞれ、/tmp/index-ceeausと/tmp/index-ceeaus-allに作られます。前の例と同じように、別のディレクトリに作成したいときは、書き換えて再度プログラムを実行してください。</p>
<h2 id="ブラウンコーパスの入手とインデックスの作成">ブラウンコーパスの入手とインデックスの作成</h2>
<p>corpora/brown 以下に次のようにしてブラウンコーパスをダウンロードし、展開します。</p>
<pre class="shell"><code>$ mkdir ${nlp4l}/corpora/brown
$ cd ${nlp4l}/corpora/brown
$ wget https://ia600503.us.archive.org/21/items/BrownCorpus/brown.zip
$ unzip brown.zip</code></pre>
<p>Windows ユーザーの方は、適宜読み替えて実行してください。</p>
<p>[Note] NLP4L の対話型シェルには、上記手順を実行するコマンド(Unix系OSのみ対応)が用意されています。</p>
<pre class="shell"><code>nlp4l&gt; downloadBrown
Successfully downloaded brown.zip
Try to execute system command: unzip -o /Users/tomoko/repo/NLP4L/corpora/brown/brown.zip -d /Users/tomoko/repo/NLP4L/corpora/brown
Success.</code></pre>
<p>次に、nlp4l プロンプトからLuceneインデックスを作成します。</p>
<pre class="shell"><code>$ bin/nlp4l
nlp4l&gt; :load examples/index_brown.scala</code></pre>
<p>プログラムの冒頭を見ていただければわかりますが、Luceneインデックスは/tmp/index-brownに作られます。前の例と同じように、別のディレクトリに作成したいときは、書き換えて再度プログラムを実行してください。</p>
<h2 id="ロイターコーパスの入手とインデックスの作成">ロイターコーパスの入手とインデックスの作成</h2>
<p>ロイターコーパスはアメリカ国立標準技術研究所（NIST）に<a href="http://trec.nist.gov/data/reuters/reuters.html">申し込む</a>ことで入手できます。ここでは同サイトに紹介されている<a href="http://www.daviddlewis.com/resources/testcollections/rcv1/">David D. Lewis博士のサイト</a>からダウンロードできるアーカイブを使ってインデックスを作成する方法を参考までにご紹介します。</p>
<p>corpora/reuters 以下に次のようにしてロイターコーパスをダウンロードし、展開します。</p>
<pre class="shell"><code>$ mkdir ${nlp4l}/corpora/reuters
$ cd ${nlp4l}/corpora/reuters
$ wget http://www.daviddlewis.com/resources/testcollections/reuters21578/reuters21578.tar.gz
$ tar xvzf reuters21578.tar.gz</code></pre>
<p>Windows ユーザーの方は、適宜読み替えて実行してください。</p>
<p>[Note] NLP4L の対話型シェルには、上記手順を実行するコマンド(Unix系OSのみ対応)が用意されています。</p>
<pre class="shell"><code>nlp4l&gt; downloadReuters
Successfully downloaded reuters21578.tar.gz
Try to execute system command: tar xzf /Users/tomoko/repo/NLP4L/corpora/reuters/reuters21578.tar.gz -C /Users/tomoko/repo/NLP4L/corpora/reuters
Success.</code></pre>
<p>次に、nlp4l プロンプトからLuceneインデックスを作成します。</p>
<pre class="shell"><code>$ bin/nlp4l
nlp4l&gt; :load examples/index_reuters.scala</code></pre>
<p>これまで同様プログラムを見れば、Luceneインデックスは/tmp/index-reutersに作られることがわかります。前の例と同じように、別のディレクトリに作成したいときは、書き換えて再度プログラムを実行してください。</p>
<h2 id="getCorpora_wiki">Wikipediaデータの入手とインデックスの作成</h2>
<p>Wikipediaデータは今NLP研究のコーパスとしても最も人気の高いものの一つとなっています。ただWikipediaの記事テキストはWikipedia独特のルールに従って書かれているため、Luceneインデックスに取り込む前になるべくテキストデータだけを抽出する前処理をする必要があり、Wikipediaデータを使う際のハードルともなっています。</p>
<p>ここでは<a href="https://github.com/diegoceccarelli/json-wikipedia">json-wikipedia</a>を使って簡単にWikipediaデータをLuceneインデックスに取り込む方法をご紹介します。</p>
<h3 id="json-wikipedia-のダウンロードとビルド">json-wikipedia のダウンロードとビルド</h3>
<p>まず<a href="https://github.com/diegoceccarelli/json-wikipedia">json-wikipedia</a>をダウンロードしますが、作業用としてworkというディレクトリを作成し、そこにjson-wikipediaをダウンロードします。</p>
<pre class="shell"><code>$ mkdir work
$ cd work
$ wget https://github.com/diegoceccarelli/json-wikipedia/archive/master.zip
$ unzip master.zip</code></pre>
<p>次に、ZIPを展開してできたディレクトリの下でjson-wikipediaをビルドします。</p>
<pre class="shell"><code>$ cd json-wikipedia-master
$ mvn assembly:assembly</code></pre>
<p>これによりtargetディレクトリ以下に作成されたJARファイルは、NLP4LからJSONに変換されたWikipediaデータをLuceneインデックスに登録する際に参照します。</p>
<pre class="shell"><code>$ ls target
archive-tmp                                    json-wikipedia-1.0.0.jar
classes                                        maven-archiver
generated-sources                              surefire-reports
generated-test-sources                         test-classes
json-wikipedia-1.0.0-jar-with-dependencies.jar</code></pre>
<h3 id="wikipediaデータのダウンロードとjsonへの変換">WikipediaデータのダウンロードとJSONへの変換</h3>
<p><a href="https://dumps.wikimedia.org/backup-index.html">Wikipediaのダウンロードサイト</a>から各国語のリンクをたどります（日本語ならjawiki、英語ならenwiki）。そしてXXwiki-YYYYMMDD-pages-articles.xml.bz2（XXは言語、YYYYMMDDは日付）という名前のファイルをダウンロードして展開します。</p>
<pre class="shell"><code>$ wget https://dumps.wikimedia.org/jawiki/20150512/jawiki-20150512-pages-articles.xml.bz2
$ bunzip2 jawiki-20150512-pages-articles.xml.bz2</code></pre>
<p>そしてjson-wikipediaを次のように実行してJSON形式に変換します。</p>
<pre class="shell"><code>$ ./scripts/convert-xml-dump-to-json.sh en jawiki-20150512-pages-articles.xml /tmp/jawiki.json</code></pre>
<p>第1引数には言語を指定します。現在json-wikipediaがサポートする言語は英語（en）やイタリア語（it）など非常に限られており、日本語のサポートはありませんので、上の例では英語（en）を第1引数に指定しています（それでも問題なく動作するようです）。日本語WikipediaのJSON形式への変換はおよそ30分程度です。</p>
<h3 id="luceneインデックスへの作成">Luceneインデックスへの作成</h3>
<p>最後にJSON形式のデータをNLP4LからLuceneインデックスに登録しますが、ビルドしてできたjson-wikipediaのJARファイルを、NLP4Lのクラスパスに含める必要があることに注意してください。</p>
<pre class="shell"><code>$ ./target/pack/bin/nlp4l -cp json-wikipedia-1.0.0-jar-with-dependencies.jar 
nlp4l&gt; </code></pre>
<p>あとは次のようにexamples/index_jawiki.scalaを実行するだけです。ただしこのサンプルプログラムは日本語Wikipedia用なので、英語など他の言語の場合はこのサンプルプログラムをコピーして他言語用のプログラムを作る必要があります。特に注意が必要なのは、プログラムから参照しているスキーマ設定ファイルです。examples/schema/jawiki.confは日本語を処理するのでJapaneseAnalyzerを指定していますが、英語などのスペースで分かち書きされている言語ではStandardAnalyzerにするのがよいでしょう。</p>
<pre class="shell"><code>nlp4l&gt; :load examples/index_jawiki.scala</code></pre>
<p>日本語Wikipediaの場合はおよそ30分程度でLuceneインデックスの作成が完了します。</p>
<h2 id="nlp4l-のスキーマについて">NLP4L のスキーマについて</h2>
<p>Luceneのインデックスは基本的にスキーマレスですが、NLP4Lではスキーマを設定できます。ここで練習用コーパスのうち、livedoorニュースコーパス（ldcc）、CEEAUS、ブラウンコーパス（brown）で定義されているスキーマを概観しましょう。</p>
<p>以下の表は各コーパスが持つフィールド名を示しています（コーパスで有効なフィールド名にチェック（x）が入っています）。</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">フィールド名</th>
<th style="text-align: center;">ldcc</th>
<th style="text-align: center;">CEEAUS</th>
<th style="text-align: center;">brown</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">file</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr class="even">
<td style="text-align: center;">type</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">x</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">cat</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">x</td>
</tr>
<tr class="even">
<td style="text-align: center;">url</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">date</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">title</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">body</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">x</td>
</tr>
<tr class="even">
<td style="text-align: center;">body_en</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">x</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">body_ws</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">x</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">body_ja</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">x</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">body_pos</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">x</td>
</tr>
</tbody>
</table>
<p>このうち、titleフィールド以下はLuceneのAnalyzerによってコーパスの登録時に文章が単語単位に分割されます。LuceneのAnalyzerは非常に高機能であり、分割だけでなくストップワードの除去、ステミング、各種文字変換などの正規化も行います。どのように分割／正規化されるかは後述します。</p>
<p>それ以外のフィールドは、登録時に単語分割が行われず、コーパスの文字列がそのまま登録されます。特にcatフィールドには文書カテゴリが記録されているため、文書分類タスクなどに利用することができます。</p>
<h3 id="ldcc-のスキーマ">ldcc のスキーマ</h3>
<p>title にはニュース記事のタイトルが、body には記事本文か格納されています。2つのフィールドとも、Lucene の JapaneseAnalyzer によって単語分割が行われます。</p>
<h3 id="ceeaus-のスキーマ">CEEAUS のスキーマ</h3>
<p>type には CEEJUS/CEECUS/CEENAS/CJEJUS の区別が、cat には ptj（大学生のアルバイト）/smk（レストラン全面禁煙）のカテゴリが入れられています。body_en と body_ws には同じ英文が格納されていますが、body_en は Lucene の StandardAnalyzer が、body_ws には Lucene の WhitespaceAnalyzer が適用されています。これらのフィールドの使い分けですが、後述の仮説検定ではbody_enを、相関分析ではbody_wsを使っています。body_ja はサブコーパス CJEJUS の日本語文章が格納されています。このフィールドには Lucene の JapaneseAnalyzer が適用されています。</p>
<h3 id="brown-のスキーマ">brown のスキーマ</h3>
<p>ブラウンコーパスの記事は次のように品詞タグが各単語ごとについています。</p>
<pre class="shell"><code>The/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta&#39;s/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn &#39;&#39;/&#39;&#39; that/cs any/dti irregularities/nns took/vbd place/nn ./.</code></pre>
<p>この文章がそのまま格納されているのが body_pos フィールドで、同じ内容だが品詞タグが取り除かれているのが body フィールドとなっています。</p>
<h1 id="useNLP">NLPツールとしての利用</h1>
<p>NLP4L を NLP ツールとして使う方法を紹介します。前述のLuceneインデックスに登録した練習用コーパスを使いますので、あらかじめ用意しておくとよいでしょう。</p>
<h2 id="単語の数を数える">単語の数を数える</h2>
<p>コーパス中に出現する単語の数を数えるのは NLP 処理の基本です。NLP4L はコーパスをLuceneのインデックスに登録してから処理を行いますが、検索エンジン（Lucene）では単語をキーにした転置インデックスというものを持っているので、単語の数を数える処理は大変得意としています。</p>
<p>ここではロイターコーパスを使って単語出現頻度を調べる方法を説明します。準備として次のプログラムをコピー＆ペーストでnlp4lプロンプトに貼り付けて実行してください。なお、コピー＆ペースト操作がしやすいように、複数行のプログラムでは nlp4l プロンプト表示を省略しています。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (1)</span>
<span class="kw">import</span> org.<span class="fu">nlp4l</span>.<span class="fu">core</span>.<span class="fu">_</span>
<span class="kw">import</span> org.<span class="fu">nlp4l</span>.<span class="fu">core</span>.<span class="fu">analysis</span>.<span class="fu">_</span>
<span class="kw">import</span> org.<span class="fu">nlp4l</span>.<span class="fu">stats</span>.<span class="fu">WordCounts</span>

<span class="co">// (2)</span>
<span class="kw">val</span> index = <span class="st">&quot;/tmp/index-reuters&quot;</span>

<span class="co">// (3)</span>
<span class="kw">val</span> reader = <span class="fu">RawReader</span>(index)</code></pre>
<p>(1)で必要なScalaプログラムのパッケージをインポートしています。このうち、単語の出現頻度はWordCountsオブジェクトを使います。(2)でロイターコーパスのLuceneインデックスディレクトリを指定しています。(3)でRawReaderに使用するLuceneインデックスディレクトリを指定してreaderを取得しています。RawReaderというのはNLP4Lでは比較的低レベルのReaderとなります。別にスキーマを管理する高レベルのIReaderというReaderもありますが、スキーマを渡す手間を省くため、ここではあえてRawReaderを使っています。</p>
<p>取得したreaderを通じて以下で単語の出現頻度を数えていきます。なお、Luceneはフィールドごとに独立した転置インデックスを持っているため、単語数をカウントするなどの処理に際してはフィールド名を指定する必要があります。以下ではロイターの記事本文を登録しているbodyフィールドを指定することにします。</p>
<h3 id="のべ語数と異なり語数">のべ語数と異なり語数</h3>
<p>最初はのべ語数です。Luceneにはもともとあるフィールドののべ語数を返す機能があるので、そのScalaラッパーであるsumTotalTermFreq()関数を使えば簡単に調べられます。</p>
<pre class="shell"><code>nlp4l&gt; val total = reader.sumTotalTermFreq(&quot;body&quot;)
total: Long = 1899819</code></pre>
<p>次は異なり語数です。異なり語数とは、単語の種類数です。転置インデックスは異なり語をキーとする構造を持っているので転置インデックスのサイズが異なり語数となります。当然Luceneでは簡単に調べることができ、そのScalaラッパーは次のようになります。</p>
<pre class="shell"><code>nlp4l&gt; val count = reader.field(&quot;body&quot;).get.terms.size
count: Int = 64625</code></pre>
<h3 id="単語ごとに数える">単語ごとに数える</h3>
<p>今度はもう少し細かく、単語ごとに数えてみましょう。WordCountsオブジェクトのcount()関数を使えば単語ごとに数えることができます。しかし、count()関数はいくつかの引数をとるため、少し準備が必要です。以下にプログラムを示します（nlp4lのプロンプトにコピー＆ペーストして実行できます）。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (4)</span>
<span class="kw">val</span> allDS = reader.<span class="fu">universalset</span>()

<span class="co">// (5)</span>
<span class="kw">val</span> analyzer = <span class="fu">Analyzer</span>(<span class="kw">new</span> org.<span class="fu">apache</span>.<span class="fu">lucene</span>.<span class="fu">analysis</span>.<span class="fu">standard</span>.<span class="fu">StandardAnalyzer</span>(<span class="kw">null</span>.<span class="fu">asInstanceOf</span>[org.<span class="fu">apache</span>.<span class="fu">lucene</span>.<span class="fu">analysis</span>.<span class="fu">util</span>.<span class="fu">CharArraySet</span>]))

<span class="co">// (6)</span>
<span class="kw">val</span> allMap = WordCounts.<span class="fu">count</span>(reader, <span class="st">&quot;body&quot;</span>, Set.<span class="fu">empty</span>, allDS, -<span class="dv">1</span>, analyzer)</code></pre>
<p>(4)ではcount()関数で使うカウント対象となるLuceneの文書番号を取得しています。ここでは全文書を対象にするので、文書の全体集合を取得するuniversalset()関数を使っています。(5)ではLuceneの標準的なAnalyzerであるStandardAnalyzerを指定してScalaのAnalyzerを作成しています。StandardAnalyzerの引数に指定しているnullは、ストップワードを使用しないということを表しています（nullを指定しないとStandardAnalyzerのデフォルトのストップワードが使われます）。(6)のcount()関数で単語ごとの出現頻度を求めています。引数に指定しているSet.emptyは、「すべての単語」を対象にカウントを求めることを指定しています。-1の部分は出現頻度の多い単語上位N個を取得したいときにそのNを指定します。-1はすべての単語を対象に調べる場合に指定します。</p>
<p>結果は表示されましたが、このままだと結果が見にくいので、表示数を絞ってみましょう。たとえばScalaのコレクション関数の機能を使って次のようにすると、&quot;g&quot;で始まる単語を10個選んで単語とその出現数を表示することができます。</p>
<pre class="shell"><code>nlp4l&gt; allMap.filter(_._1.startsWith(&quot;g&quot;)).take(10).foreach(println(_))
(generalize,1)
(gress,1)
(germans,18)
(goiporia,2)
(garcin,2)
(granma,7)
(gorbachev&#39;s,10)
(gamble,9)
(grains,110)
(gienow,1)</code></pre>
<p>allMapの結果のすべての数を足し合わせてみます（以下のプログラムはWordCountsのtotalCount()関数とやっていることは同じです）。</p>
<pre class="shell"><code>nlp4l&gt; allMap.values.sum
res2: Long = 1899819</code></pre>
<p>これは最初に調べたのべ語数と確かに一致していることがわかります。また、allMapの大きさは異なり語数と一致しているはずです。やってみましょう。</p>
<pre class="shell"><code>nlp4l&gt; allMap.size
res3: Int = 64625</code></pre>
<p>こちらも一致しました。</p>
<p>上のcount()ではSet.emptyを渡して全単語を対象に出現頻度をカウントしていましたが、Set.emptyの代わりに特定の単語集合を与えることで、その単語集合だけを対象にカウントできます（特定の単語の数だけを数えたいこともNLPではよくあります）。やってみましょう。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> whWords = Set(<span class="st">&quot;when&quot;</span>, <span class="st">&quot;where&quot;</span>, <span class="st">&quot;who&quot;</span>, <span class="st">&quot;what&quot;</span>, <span class="st">&quot;which&quot;</span>, <span class="st">&quot;why&quot;</span>)
<span class="kw">val</span> whMap = WordCounts.<span class="fu">count</span>(reader, <span class="st">&quot;body&quot;</span>, whWords, allDS, -<span class="dv">1</span>, analyzer)
whMap.<span class="fu">foreach</span>(<span class="fu">println</span>(_))</code></pre>
<p>結果は次のようになります。</p>
<pre class="shell"><code>nlp4l&gt; whMap.foreach(println(_))
(why,125)
(what,850)
(who,1618)
(which,7556)
(where,507)
(when,1986)</code></pre>
<h3 id="カテゴリごとに数える">カテゴリごとに数える</h3>
<p>では次に単語出現頻度をカテゴリごとに区別して求める方法を見てみましょう。たとえばNLPタスクのひとつ、文書分類ではカテゴリに分類するにあたってその学習データとしてカテゴリごとの単語出現頻度を使うことがあります。そのようなときに使えるテクニックです。</p>
<p>ここの例ではロイターコーパスを使っていますが、placesフィールドをカテゴリの代わりに使ってみたいと思います。そのためにはまず、placesフィールドに入っている単語を次のようにして調べます。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (7)</span>
reader.<span class="fu">terms</span>(<span class="st">&quot;places&quot;</span>).<span class="fu">get</span>.<span class="fu">map</span>(_.<span class="fu">text</span>)

<span class="co">// (8) 整形して表示したい場合</span>
reader.<span class="fu">terms</span>(<span class="st">&quot;places&quot;</span>).<span class="fu">get</span>.<span class="fu">map</span>(_.<span class="fu">text</span>).<span class="fu">foreach</span>(<span class="fu">println</span>(_))</code></pre>
<p>(7)または(8)を実行すると、placesフィールドに登録されているすべての単語一覧が表示されます。ここではusaとjapanに注目しましょう。(9)のように実行してそれぞれの文書部分集合を取得します。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (9)</span>
<span class="kw">val</span> usDS = reader.<span class="fu">subset</span>(<span class="fu">TermFilter</span>(<span class="st">&quot;places&quot;</span>, <span class="st">&quot;usa&quot;</span>))
<span class="kw">val</span> jpDS = reader.<span class="fu">subset</span>(<span class="fu">TermFilter</span>(<span class="st">&quot;places&quot;</span>, <span class="st">&quot;japan&quot;</span>))</code></pre>
<p>最後にcount()関数に(9)で求めたそれぞれの部分集合を渡してusaのカウントとjapanのカウントを求めますが、ここでは簡単にwarとpeaceの2語だけのカウントを求めることにします(10)。</p>
<pre class="shell"><code>// (10)
nlp4l&gt; WordCounts.count(reader, &quot;body&quot;, Set(&quot;war&quot;, &quot;peace&quot;), usDS, -1, analyzer)
res22: Map[String,Long] = Map(war -&gt; 199, peace -&gt; 14)

nlp4l&gt; WordCounts.count(reader, &quot;body&quot;, Set(&quot;war&quot;, &quot;peace&quot;), jpDS, -1, analyzer)
res23: Map[String,Long] = Map(war -&gt; 75, peace -&gt; 2)</code></pre>
<p>これでwarとpeaceの2語に対する、placesがusaの場合とjapanの場合でのカウントがそれぞれ求めることができました。めでたしめでたし・・・としていいでしょうか。</p>
<p>実はplacesフィールドは、地名が複数入っている可能性があります。つまり、usaとjapanの記事が重なっている可能性があります。確かめてみましょう。usDSとjpDSはScalaのSetコレクションオブジェクトですから、&amp;演算子（関数）を使って両者の積集合を簡単に求めることができます。</p>
<pre class="shell"><code>nlp4l&gt; (usDS &amp; jpDS).size
res24: Int = 452</code></pre>
<p>sizeを使って積集合の大きさを求めています。たしかに重複があるようです。この場合、(11)(12)のように、ScalaのSetの演算子（関数）の&amp;~を使って集合の差を使えば、重複のない部分に関して単語出現頻度を求めることができます（ただしそのままだとSortedSetには&amp;~演算子（関数）がないので、toSetを使ってSetに変換しています）。</p>
<pre class="shell"><code>nlp4l&gt; // (11) placesにusaの値を持つがjapanの値を持たない記事を対象とする
nlp4l&gt; WordCounts.count(reader, &quot;body&quot;, Set(&quot;war&quot;, &quot;peace&quot;), usDS.toSet &amp;~ jpDS.toSet, -1, analyzer)
res25: Map[String,Long] = Map(war -&gt; 140, peace -&gt; 13)

nlp4l&gt; // (12) placesにjapanの値を持つがusaの値を持たない記事を対象とする
nlp4l&gt; WordCounts.count(reader, &quot;body&quot;, Set(&quot;war&quot;, &quot;peace&quot;), jpDS.toSet &amp;~ usDS.toSet, -1, analyzer)
res26: Map[String,Long] = Map(war -&gt; 16, peace -&gt; 1)</code></pre>
<h3 id="単語カウントの視覚化experimental">単語カウントの視覚化（experimental）</h3>
<p>これまでいくつかの視点で単語の数を数えてきました。では最後に、単語カウントデータを視覚化してみましょう。Excelなどを使えば視覚化できますが、NLP4Lでは試験的に簡単なチャート表示ツールを提供していますのでここではそれを使います。チャート表示ツールは現在 experimental です。改善のため将来は機能や使い方が変わる可能性があります。</p>
<p>前述の単語カウント結果をあらためて実行し、変数に代入します。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> usMap = WordCounts.<span class="fu">count</span>(reader, <span class="st">&quot;body&quot;</span>, Set(<span class="st">&quot;war&quot;</span>, <span class="st">&quot;peace&quot;</span>), usDS, -<span class="dv">1</span>, analyzer)
<span class="kw">val</span> jpMap = WordCounts.<span class="fu">count</span>(reader, <span class="st">&quot;body&quot;</span>, Set(<span class="st">&quot;war&quot;</span>, <span class="st">&quot;peace&quot;</span>), jpDS, -<span class="dv">1</span>, analyzer)</code></pre>
<p>次に、チャート表示のためのパッケージをインポートし(11)、チャート表示のプレゼンテーションを取得します(12)。その際、上で取得したデータモデルをプレゼンテーションに渡しています。プレゼンテーションにデータモデルを渡す際は、凡例表示のためのラベルも必要です。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (11)</span>
<span class="kw">import</span> org.<span class="fu">nlp4l</span>.<span class="fu">gui</span>.<span class="fu">_</span>

<span class="co">// (12)</span>
<span class="kw">val</span> presentation = <span class="fu">BarChart</span>(List((<span class="st">&quot;US&quot;</span>,usMap), (<span class="st">&quot;Japan&quot;</span>,jpMap)))

<span class="co">// (13)</span>
<span class="kw">val</span> server = <span class="kw">new</span> <span class="fu">SimpleHttpServer</span>(presentation)
server.<span class="fu">service</span></code></pre>
<p>チャート表示はWebサーバから配信されますので、(13)のようにプレゼンテーションを引数にして簡易Webサーバを作成して起動します。起動すると、次のメッセージがコンソールに表示されます。</p>
<pre class="shell"><code>nlp4l&gt; server.service
WARNING: This function is experimental and might change in incompatible ways in the future release.

To see the chart, access this URL -&gt; http://localhost:6574/chart
To shutdown the server, access this URL -&gt; http://localhost:6574/shutdown</code></pre>
<p>表示されている<a href="http://localhost:6574/chart">URL</a>にモダンなWebブラウザからアクセスすると、棒グラフが表示されます。簡易Webサーバを停止するには、<a href="http://localhost:6574/shutdown" class="uri">http://localhost:6574/shutdown</a>にアクセスします。</p>
<figure>
<img src="barchart_wc.png" alt="カテゴリごとの単語出現頻度" /><figcaption>カテゴリごとの単語出現頻度</figcaption>
</figure>
<p>以上のチャート表示のコードは examples/chart_experimental.scala として1つのスクリプトにまとめてあります。</p>
<h2 id="隠れマルコフモデル">隠れマルコフモデル</h2>
<p>NLP4L では、ラベルつき訓練データから隠れマルコフモデルを学習することができる HmmModel クラスが提供されています。HmmModel と HmmModelIndexer はともに次の HmmModelSchema で定義されている Luceneインデックスのスキーマを参照しています。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">trait</span> HmmModelSchema {
  <span class="kw">def</span> <span class="fu">schema</span>(): Schema = {
    <span class="kw">val</span> analyzer = <span class="fu">Analyzer</span>(<span class="kw">new</span> org.<span class="fu">apache</span>.<span class="fu">lucene</span>.<span class="fu">analysis</span>.<span class="fu">core</span>.<span class="fu">WhitespaceAnalyzer</span>)
    <span class="kw">val</span> builder = <span class="fu">AnalyzerBuilder</span>()
    builder.<span class="fu">withTokenizer</span>(<span class="st">&quot;whitespace&quot;</span>)
    builder.<span class="fu">addTokenFilter</span>(<span class="st">&quot;shingle&quot;</span>, <span class="st">&quot;minShingleSize&quot;</span>, <span class="st">&quot;2&quot;</span>, <span class="st">&quot;maxShingleSize&quot;</span>, <span class="st">&quot;2&quot;</span>, <span class="st">&quot;outputUnigrams&quot;</span>, <span class="st">&quot;false&quot;</span>)
    <span class="kw">val</span> analyzer2g = builder.<span class="fu">build</span>
    <span class="kw">val</span> fieldTypes = Map(
      <span class="st">&quot;begin&quot;</span> -&gt; <span class="fu">FieldType</span>(analyzer, <span class="kw">true</span>, <span class="kw">true</span>, <span class="kw">true</span>, <span class="kw">true</span>),
      <span class="st">&quot;class&quot;</span> -&gt; <span class="fu">FieldType</span>(analyzer, <span class="kw">true</span>, <span class="kw">true</span>, <span class="kw">true</span>, <span class="kw">true</span>),
      <span class="st">&quot;class_2g&quot;</span> -&gt; <span class="fu">FieldType</span>(analyzer2g, <span class="kw">true</span>, <span class="kw">true</span>, <span class="kw">true</span>, <span class="kw">true</span>),
      <span class="st">&quot;word_class&quot;</span> -&gt; <span class="fu">FieldType</span>(analyzer, <span class="kw">true</span>, <span class="kw">true</span>, <span class="kw">true</span>, <span class="kw">true</span>),
      <span class="st">&quot;word&quot;</span> -&gt; <span class="fu">FieldType</span>(analyzer, <span class="kw">true</span>, <span class="kw">true</span>, <span class="kw">true</span>, <span class="kw">true</span>)
    )
    <span class="kw">val</span> analyzerDefault = analyzer
    Schema(analyzerDefault, fieldTypes)
  }
}</code></pre>
<p>隠れマルコフモデルでは、ある状態（クラス）からある状態へ遷移する確率とある状態における記号（単語）の出力確率を使います。NLP4Lではこれらの確率を HmmModelSchema で定義されるスキーマを持つ Luceneインデックスを使って求めます。状態遷移確率は class と class_2g フィールドのクラス出現数を使います。class_2g フィールドは Lucene の ShingleFilter を使ってクラス2グラムを記録しているフィールドです。class フィールドは単純にクラスを記録しているフィールドです。この2つのフィールドを使って P( vb | nn ) すなわち品詞 nn のあとに品詞 vb が出現する確率を求めることを考えます。これは実は非常に簡単で、先の単語の数を数える totalTermFreq() を使って次のように計算できます。</p>
<pre class="math"><code>P( vb | nn ) = &quot;nn vb&quot;.totalTermFreq() / &quot;nn&quot;.totalTermFreq()</code></pre>
<p>&quot;nn vb&quot; という2つのクラスの連続は class_2g フィールドを参照します。&quot;nn&quot; は class フィールドを参照します。同様にクラス nn における単語 program の出力確率は次のように計算できます。</p>
<pre class="math"><code>P( program | nn ) = &quot;program_nn&quot;.totalTermFreq() / &quot;nn&quot;.totalTermFreq()</code></pre>
<p>ここで &quot;program_nn&quot; は単語 program とクラス nn が同時に観測された場合の Luceneインデックスに登録する文字列です。これは word_class フィールドに記録されます。</p>
<p>begin フィールドには Lucene ドキュメントの最初のクラスが記録されます。このフィールドの totalTermFreq() を使えば、各クラスの初期状態確率分布が計算できます。</p>
<p>それでは HmmModel を使った具体例を見てみましょう。</p>
<h3 id="英語の品詞タグ付け">英語の品詞タグ付け</h3>
<p>隠れマルコフモデルを応用した成功例には英語の品詞タグ付けが知られています。ブラウンコーパスの英文には品詞タグがつけられていますので、ここから HmmModel を学習して、未知の英文に品詞タグをつけてみましょう。ブラウンコーパスを用意した状態で、次のようにサンプルスクリプトを実行します。</p>
<pre class="shell"><code>nlp4l&gt; :load examples/hmm_postagger.scala</code></pre>
<p>プログラムの最後に学習後のLuceneインデックスモデルを用いて未知の英文に品詞タグ付けを行っています。たとえば、&quot;i like to go to france .&quot; という英文の品詞タグ付け結果は次のように出力されます。</p>
<pre class="sourceCode scala"><code class="sourceCode scala">res8: Seq[org.<span class="fu">nlp4l</span>.<span class="fu">lm</span>.<span class="fu">Token</span>] = List(<span class="fu">Token</span>(i,ppss), <span class="fu">Token</span>(like,vb), <span class="fu">Token</span>(to,to), <span class="fu">Token</span>(go,vb), <span class="fu">Token</span>(to,in), <span class="fu">Token</span>(france,np), <span class="fu">Token</span>(.,.))</code></pre>
<p>ではサンプルプログラム冒頭の学習部分を見てみましょう。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (1)</span>
<span class="kw">val</span> index = <span class="st">&quot;/tmp/index-brown-hmm&quot;</span>

<span class="co">// (2)</span>
<span class="kw">val</span> c: PathSet[Path] = <span class="fu">Path</span>(<span class="st">&quot;corpora&quot;</span>, <span class="st">&quot;brown&quot;</span>, <span class="st">&quot;brown&quot;</span>).<span class="fu">children</span>()

<span class="co">// (3)</span>
<span class="kw">val</span> indexer = <span class="fu">HmmModelIndexer</span>(index)
c.<span class="fu">filter</span>{ e =&gt;
  <span class="kw">val</span> s = e.<span class="fu">name</span>
  <span class="kw">val</span> c = s.<span class="fu">charAt</span>(s.<span class="fu">length</span> - <span class="dv">1</span>)
  c &gt;= &#39;<span class="dv">0</span>&#39; &amp;&amp; c &lt;= &#39;<span class="dv">9</span>&#39;
}.<span class="fu">foreach</span>{ f =&gt;
  <span class="kw">val</span> source = Source.<span class="fu">fromFile</span>(f.<span class="fu">path</span>, <span class="st">&quot;UTF-8&quot;</span>)
  source.<span class="fu">getLines</span>().<span class="fu">map</span>(_.<span class="fu">trim</span>).<span class="fu">filter</span>(_.<span class="fu">length</span> &gt; <span class="dv">0</span>).<span class="fu">foreach</span> { g =&gt;
    <span class="kw">val</span> pairs = g.<span class="fu">split</span>(<span class="st">&quot;</span><span class="ch">\\</span><span class="st">s+&quot;</span>)
    <span class="kw">val</span> doc = pairs.<span class="fu">map</span>{h =&gt; h.<span class="fu">split</span>(<span class="st">&quot;/&quot;</span>)}.<span class="fu">filter</span>{_.<span class="fu">length</span>==<span class="dv">2</span>}.<span class="fu">map</span>{i =&gt; (<span class="fu">i</span>(<span class="dv">0</span>).<span class="fu">toLowerCase</span>(), <span class="fu">i</span>(<span class="dv">1</span>))}
    indexer.<span class="fu">addDocument</span>(doc)
  }
}

<span class="co">// (4)</span>
indexer.<span class="fu">close</span>()

<span class="co">// (5)</span>
<span class="kw">val</span> model = <span class="fu">HmmModel</span>(index)</code></pre>
<p>最初に (1) でブラウンコーパスを登録するLuceneインデックスを指定しています。(2) でブラウンコーパスのディレクトリを指定しています。これは下の(3) でファイルをひとつずつ取り出すのに参照されます。(3) で HmmModelIndexer を作成し、ブラウンコーパスのファイルの1行をLuceneドキュメントの1つとみなしてaddDocument()で HmmModelIndexer に追加しています。追加するドキュメントの形式は、単語と品詞のタプルの配列です。</p>
<p>Luceneインデックスを作成し終わったら(4)でクローズします。このようにして作成したLuceneインデックスを(5)で HmmModel で読み込んだときに隠れマルコフモデルが計算されます。</p>
<p>英語の品詞タグ付けでモデルを使う場合は、次のように HmmTagger にモデルを適用し、HmmTagger の tokens() を呼び出します。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> tagger = <span class="fu">HmmTagger</span>(model)

tagger.<span class="fu">tokens</span>(<span class="st">&quot;i like to go to france .&quot;</span>)
tagger.<span class="fu">tokens</span>(<span class="st">&quot;you executed lucene program .&quot;</span>)
tagger.<span class="fu">tokens</span>(<span class="st">&quot;nlp4l development members may be able to present better keywords .&quot;</span>)</code></pre>
<p>実行結果は次のように、単語とクラス（品詞）を要素に持つTokenオブジェクトのListとして返されます。</p>
<pre class="shell"><code>res23: Seq[org.nlp4l.lm.Token] = List(Token(i,ppss), Token(like,vb), Token(to,to), Token(go,vb), Token(to,in), Token(france,np), Token(.,.))
res24: Seq[org.nlp4l.lm.Token] = List(Token(you,ppo-tl), Token(executed,vbn), Token(lucene,X), Token(program,nil), Token(.,.))
res25: Seq[org.nlp4l.lm.Token] = List(Token(nlp4l,X), Token(development,nn), Token(members,nns), Token(may,md), Token(be,be), Token(able,jj), Token(to,to), Token(present,vb), Token(better,rbr), Token(keywords,X), Token(.,.-hl))</code></pre>
<h3 id="カタカナ語からの英単語推定">カタカナ語からの英単語推定</h3>
<p>NLP4L にはカタカナ語とその元になった英単語のペアに、独自にアライメントをつけた訓練データが付属しています（train_data/alpha_katakana_aligned.txt）。</p>
<pre class="shell"><code>$ head train_data/alpha_katakana_aligned.txt 
アaカcaデdeミーmy
アaクcセceンnトt
アaクcセceスss
アaクcシciデdeンnトt
アaクcロroバッbaトt
アaクcショtioンn
アaダdaプpターter
アaフfリriカca
エaアirバbuスs
アaラlaスsカka</code></pre>
<p>このデータを使って、カタカナ部分を単語に、アルファベット部分を品詞に見立てて隠れマルコフモデルを学習することを考えてみましょう。すると未知のカタカナ語から英単語を予測する事ができます。それを実装したのがexamples/trans_katakana_alpha.scala です。</p>
<pre class="shell"><code>nlp4l&gt; :load examples/trans_katakana_alpha.scala</code></pre>
<p>前の examples/hmm_postagger.scala とほぼ同じプログラムなので興味がある方は読み解いてみてください。</p>
<p>同じくプログラムの最後は学習したモデルを使って、未知のカタカナ語から英単語を推定した結果を表示しています。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> tokenizer = <span class="fu">HmmTokenizer</span>(model)

tokenizer.<span class="fu">tokens</span>(<span class="st">&quot;アクション&quot;</span>)
tokenizer.<span class="fu">tokens</span>(<span class="st">&quot;プログラム&quot;</span>)
tokenizer.<span class="fu">tokens</span>(<span class="st">&quot;ポイント&quot;</span>)
tokenizer.<span class="fu">tokens</span>(<span class="st">&quot;テキスト&quot;</span>)
tokenizer.<span class="fu">tokens</span>(<span class="st">&quot;コミュニケーション&quot;</span>)
tokenizer.<span class="fu">tokens</span>(<span class="st">&quot;エントリー&quot;</span>)</code></pre>
<p>この実行結果は次のようになります。</p>
<pre class="shell"><code>res35: Seq[org.nlp4l.lm.Token] = List(Token(ア,a), Token(ク,c), Token(ショ,tio), Token(ン,n))
res36: Seq[org.nlp4l.lm.Token] = List(Token(プ,p), Token(ロ,ro), Token(グ,g), Token(ラ,ra), Token(ム,m))
res37: Seq[org.nlp4l.lm.Token] = List(Token(ポ,po), Token(イ,i), Token(ン,n), Token(ト,t))
res38: Seq[org.nlp4l.lm.Token] = List(Token(テ,te), Token(キス,x), Token(ト,t))
res39: Seq[org.nlp4l.lm.Token] = List(Token(コ,co), Token(ミュ,mmu), Token(ニ,ni), Token(ケー,ca), Token(ショ,tio), Token(ン,n))
res40: Seq[org.nlp4l.lm.Token] = List(Token(エ,e), Token(ン,n), Token(ト,t), Token(リー,ree))</code></pre>
<h2 id="連語分析モデル">連語分析モデル</h2>
<p>NLP4Lでは、コーパス中に発生するある単語に注目したとき、その単語の前後に発生する単語を発生頻度付きで分析することができます。この分析を行うデータモデルをNLP4Lでは連語分析モデル（CollocationalAnalysisModel）と名付けました。連語分析モデルを使うと、ある動詞と共起しやすい前置詞などがわかります。たとえば英語学習者が英語コーパスを連語分析モデルを使って調べることで、よく使われる言い回しを得ることができるでしょう。</p>
<p>examples/colloc_analysis_brown.scala はブラウンコーパスを分析して単語&quot;found&quot;の前後に出現しやすい単語を出現順に表示します。examples/colloc_analysis_brown.scala の出力を見やすく表にすると、次のようになります。</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">3単語前</th>
<th style="text-align: center;">2単語前</th>
<th style="text-align: center;">1単語前</th>
<th style="text-align: center;">注目している単語</th>
<th style="text-align: center;">1単語後</th>
<th style="text-align: center;">2単語後</th>
<th style="text-align: center;">3単語後</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">the(36)</td>
<td style="text-align: center;">to(26)</td>
<td style="text-align: center;">be(60)</td>
<td style="text-align: center;">found</td>
<td style="text-align: center;">in(77)</td>
<td style="text-align: center;">the(46)</td>
<td style="text-align: center;">the(28)</td>
</tr>
<tr class="even">
<td style="text-align: center;">and(13)</td>
<td style="text-align: center;">he(20)</td>
<td style="text-align: center;">he(50)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">a(43)</td>
<td style="text-align: center;">in(24)</td>
<td style="text-align: center;">to(19)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">a(12)</td>
<td style="text-align: center;">the(17)</td>
<td style="text-align: center;">was(32)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">that(42)</td>
<td style="text-align: center;">a(13)</td>
<td style="text-align: center;">in(15)</td>
</tr>
<tr class="even">
<td style="text-align: center;">of(11)</td>
<td style="text-align: center;">and(15)</td>
<td style="text-align: center;">and(26)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">the(38)</td>
<td style="text-align: center;">be(13)</td>
<td style="text-align: center;">of(13)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">is( 9)</td>
<td style="text-align: center;">have(13)</td>
<td style="text-align: center;">been(26)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">it(28)</td>
<td style="text-align: center;">to(12)</td>
<td style="text-align: center;">and( 9)</td>
</tr>
<tr class="even">
<td style="text-align: center;">was( 8)</td>
<td style="text-align: center;">can(10)</td>
<td style="text-align: center;">i(26)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">to(28)</td>
<td style="text-align: center;">and(11)</td>
<td style="text-align: center;">a( 8)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">he( 7)</td>
<td style="text-align: center;">i(10)</td>
<td style="text-align: center;">she(23)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">himself(13)</td>
<td style="text-align: center;">of(11)</td>
<td style="text-align: center;">with( 8)</td>
</tr>
<tr class="even">
<td style="text-align: center;">in( 7)</td>
<td style="text-align: center;">has( 9)</td>
<td style="text-align: center;">have(22)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">out(12)</td>
<td style="text-align: center;">he( 7)</td>
<td style="text-align: center;">had( 7)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">that( 7)</td>
<td style="text-align: center;">of( 9)</td>
<td style="text-align: center;">had(21)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">him( 9)</td>
<td style="text-align: center;">at( 6)</td>
<td style="text-align: center;">be( 6)</td>
</tr>
<tr class="even">
<td style="text-align: center;">as( 5)</td>
<td style="text-align: center;">could( 8)</td>
<td style="text-align: center;">they(19)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">myself( 9)</td>
<td style="text-align: center;">had( 5)</td>
<td style="text-align: center;">men( 4)</td>
</tr>
</tbody>
</table>
<p>&quot;found in&quot; というフレーズが多く発生していることがわかります。また、注目している単語 &quot;found&quot; の前の単語もわかるので、&quot;be found&quot; や &quot;to be found&quot;, &quot;can be found&quot; といったフレーズも発生している可能性がうかがえます。</p>
<h2 id="ジップの法則を確認する">ジップの法則を確認する</h2>
<h2 id="仮説検定">仮説検定</h2>
<p>先行研究では非母語話者の書いた英語にはIやyouが多用されがちで、書き手・読み手の可視性が母語話者以上に顕著であるとされます（[1]の3章、[2]）。</p>
<pre class="shell"><code>[2] Petch-Tyson, S. (1998). Writer/reader visibility in EFL written discourse. In S. Granger (Ed.), Learner English on Computer (pp. 107-118). London, UK: Longman.</code></pre>
<p>そこでここではCEEAUSのデータを使って、日本人英語学習者は英作文で1、2人称代名詞を過剰使用していないことを帰無仮説としてα=.05でカイ2乗検定を行ってみましょう。インデックスデータとしては/tmp/index-ceeaus-allを使いますので、前述のプログラム examples/index_ceeaus_all.scala をあらかじめ実行してインデックスを作成しておきます。次に同じくnlp4lプロンプトから examples/chisquare_test_ceeaus.scala を実行します。</p>
<pre class="shell"><code>nlp4l&gt; :load examples/chisquare_test_ceeaus.scala</code></pre>
<p>実行すると、次のような一覧が表示されます（カイ2乗統計量が[1]の3章の結果と微妙に異なりますが、単語数が違っているためです。どのようなトークナイザを使用したかによって単語数は異なってきます）。</p>
<pre class="shell"><code>        CEEJUS  CEENAS  chi square
==============================================
       i         4,367     384   324.8068
      my           526     101     1.4384
      me           230      28     8.5172
     you           802      73    55.1096
    your           160      24     2.7664</code></pre>
<p>2種類のカテゴリについて検定を多重に繰り返すため、ボンフェローニ補正を行いαb=.025を用います。統計の参考書に付録としてついているカイ2乗分布表を見ると、自由度=1、αb=.025の限界値は5.02389ですから、この値を超えるi、me、youの語については帰無仮説が棄却され、日本人英語学習者はこれらの語を英語母語話者に比べて過剰使用している（有意差がある）といえることがわかりました（書籍[1]の結果と同じです）。</p>
<p>では仮説検定を行うプログラムを順に見てみましょう。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (1)</span>
<span class="kw">val</span> index = <span class="st">&quot;/tmp/index-ceeaus-all&quot;</span>

<span class="co">// (2)</span>
<span class="kw">val</span> schema = SchemaLoader.<span class="fu">load</span>(<span class="st">&quot;examples/schema/ceeaus.conf&quot;</span>)

<span class="co">// (3)</span>
<span class="kw">val</span> reader = <span class="fu">IReader</span>(index, <span class="fu">schema</span>())</code></pre>
<p>(1)でLuceneのインデックスを指定しています。(2)はLuceneインデックスのスキーマを設定しています。(3)は(1)(2)で設定したインデックスとスキーマ情報を渡し、IReaderオブジェクトを作成して定数readerに代入しています。readerを通じてインデックス（コーパス）にアクセスできます。</p>
<p>次の行からは早速そのreaderを使ってインデックスのサブセットを取得しています。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (4)</span>
<span class="kw">val</span> docSetJUS = reader.<span class="fu">subset</span>(<span class="fu">TermFilter</span>(<span class="st">&quot;file&quot;</span>, <span class="st">&quot;ceejus_all.txt&quot;</span>))
<span class="kw">val</span> docSetNAS = reader.<span class="fu">subset</span>(<span class="fu">TermFilter</span>(<span class="st">&quot;file&quot;</span>, <span class="st">&quot;ceenas_all.txt&quot;</span>))</code></pre>
<p>インデックスのサブセットを取得するには、IReaderのsubset()関数にFilterを渡して行います。ここではFilterとしてTermFilterを使っています。TermFilterは2つの引数を取り、最初の引数でLuceneインデックスのフィールド名を指定し、2つめの引数でそのフィールド値を指定します。このようにすることで、Luceneインデックスの中から該当するフィールド値を持つ文書セットだけを指定できます。(4)の1行目はファイル名がceejus_all.txtとなっているものを指定しているので、日本人英語学習者の英作文だけを取り出しています。2行目はファイル名がceenas_all.txtとなっているものを指定しているので、英語母語話者の英作文を対象にしています。</p>
<p>次にWordCountsオブジェクトのtotalCount()関数を使ってbody_enフィールド中の全単語数を数えています。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (5)</span>
<span class="kw">val</span> totalCountJUS = WordCounts.<span class="fu">totalCount</span>(reader, <span class="st">&quot;body_en&quot;</span>, docSetJUS)
<span class="kw">val</span> totalCountNAS = WordCounts.<span class="fu">totalCount</span>(reader, <span class="st">&quot;body_en&quot;</span>, docSetNAS)</code></pre>
<p>このとき、(4)で取得した文書セットを第3引数で渡すことで、それぞれ日本人英語学習者と英語母語話者のカウントを分けて取得するようにしています。</p>
<p>次にwordsに今回注目する単語のリストを保持します。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (6)</span>
<span class="kw">val</span> words = List(<span class="st">&quot;i&quot;</span>, <span class="st">&quot;my&quot;</span>, <span class="st">&quot;me&quot;</span>, <span class="st">&quot;you&quot;</span>, <span class="st">&quot;your&quot;</span>)</code></pre>
<p>次にその注目する単語について、count()関数を使って日本人英語学習者と英語母語話者それぞれの単語数を単語ごとに数えます（単語別のカウントがMap[String,Long]で得られます）。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (7)</span>
<span class="kw">val</span> wcJUS = WordCounts.<span class="fu">count</span>(reader, <span class="st">&quot;body_en&quot;</span>, words.<span class="fu">toSet</span>, docSetJUS)
<span class="kw">val</span> wcNAS = WordCounts.<span class="fu">count</span>(reader, <span class="st">&quot;body_en&quot;</span>, words.<span class="fu">toSet</span>, docSetNAS)</code></pre>
<p>最後にStatsオブジェクトのchiSquare()関数を使ってカイ2乗統計量を求めています。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (8)</span>
<span class="fu">println</span>(<span class="st">&quot;</span><span class="ch">\t\t</span><span class="st">CEEJUS</span><span class="ch">\t</span><span class="st">CEENAS</span><span class="ch">\t</span><span class="st">chi square&quot;</span>)
<span class="fu">println</span>(<span class="st">&quot;==============================================&quot;</span>)
words.<span class="fu">foreach</span>{ w =&gt;
  <span class="kw">val</span> countJUS = wcJUS.<span class="fu">getOrElse</span>(w, <span class="fl">0.</span>toLong)
  <span class="kw">val</span> countNAS = wcNAS.<span class="fu">getOrElse</span>(w, <span class="fl">0.</span>toLong)
  <span class="kw">val</span> cs = Stats.<span class="fu">chiSquare</span>(countJUS, totalCountJUS - countJUS, countNAS, totalCountNAS - countNAS, <span class="kw">true</span>)
  <span class="fu">println</span>(<span class="st">&quot;%8s</span><span class="ch">\t</span><span class="st">%,6d</span><span class="ch">\t</span><span class="st">%,6d</span><span class="ch">\t</span><span class="st">%9.4f&quot;</span><span class="fu">.format</span>(w, countJUS, countNAS, cs))
}

<span class="co">// (9)</span>
reader.<span class="fu">close</span></code></pre>
<p>chiSquare()関数の第1、3引数にはそれぞれの単語のカウント数を、第2、4引数にはそれ以外の単語のカウント数を渡します。また、第5引数にはイェーツ補正を行う（true）か否か（false）を指定します。</p>
<p>プログラムの最後にIReaderをclose()します(9)。</p>
<p>同じように、ブラウンコーパスを使って特定の単語の使用頻度に有意差があるかどうかを見てみましょう。書籍[3]2.1.3には法助動詞couldとwillの使用頻度が、前者はromanceカテゴリにおいて高く、後者はnewsカテゴリにおいて高いという結果が示されています。<a href="http://www.nltk.org/book/ch02.html">NLTK web</a></p>
<pre class="shell"><code>[3] Natural Language Processing with Python
Steven Bird, Ewan Klein, Edward Loper
O&#39;Reilly

入門自然言語処理
萩原正人、中山敬広、水野貴明
オライリー・ジャパン</code></pre>
<p>ここではもう少し突っ込んで仮説検定を行い、この使用頻度に有意差があるか計算します。同様にαb=.025を用います。実行結果は次のようになります。</p>
<pre class="shell"><code>nlp4l&gt; :load examples/chisquare_test_brown.scala
             romance    news    chi square
==============================================
   could         195      87     101.2919
    will          49     390     148.4053</code></pre>
<p>どちらも自由度=1、αb=.025の限界値5.02389を超えているので有意差があるといえます。プログラムは前掲のものとほとんど同じなので解説は省略します。</p>
<h2 id="相関分析">相関分析</h2>
<p>接続副詞は文内要素の論理的な結束性を担う重要な役割を果たしています（[1]の4章）。ここではCEEAUSのCEEJUS（日本人英語学習者）、CEECUS（中国人英語学習者）、CEENAS（英語母語話者）のサブコーパス間で接続副詞の使用頻度を比較し、どの程度の関係性が存在するのか見てみましょう。仮にどのような書き手であっても接続表現の使用パターンが安定しているのであれば、書き手間に高い相関関係が見えるでしょうし、逆に母語話者かどうかで使用頻度が変わるのであれば、相関係数は低くなるでしょう。</p>
<p>では早速プログラムを実行してみます。同じくインデックスデータとしては/tmp/index-ceeaus-allを使いますので、前述のプログラム examples/index_ceeaus_all.scala をあらかじめ実行してインデックスを作成しておきます。次にnlp4lプロンプトから examples/correlation_analysis_ceeaus.scala を実行します。</p>
<pre class="shell"><code>nlp4l&gt; :load examples/correlation_analysis_ceeaus.scala</code></pre>
<p>実行すると2つの表が表示されます。1つめの表は次の通りです。</p>
<pre class="shell"><code>word counts
========================================
    word    CEEJUS    CEECUS    CEENAS
  besides,      21         5         2
nevertheless,    2         1         1
     also,      18         5         8
 moreover,      68         8         4
  however,     165        18        33
therefore,      86         7        13
       so,     242         9         5</code></pre>
<p>wordの列にある単語が接続副詞です。各接続副詞の後ろにはカンマ（&quot;,&quot;）がついていますが、これは他の一般副詞と区別するため、接続副詞をコーパスから検索する際に後ろにカンマを伴っている単語を探すところから来ています。数字は各サブコーパスにおける出現頻度を示しています。ただし出現頻度を単純に比較するだけでは各サブコーパス間で相関性があるのかはわかりません。そこで2つめの表を見てみます。</p>
<pre class="shell"><code>Correlation Coefficient
================================
    CEEJUS  CEECUS  CEENAS
CEEJUS  1.000   0.690   0.433
CEECUS  0.690   1.000   0.886
CEENAS  0.433   0.886   1.000</code></pre>
<p>2つめの表は単相関行列表を示しています。この結果から、日本人英語学習者と英語母語話者は中程度の正の相関が認められ（r=.43）、中国人英語学習者と英語母語話者は強い正の相関が認められる（r=.89）ことがわかりました。また日本人英語学習者と中国人英語学習者は中程度の正の相関があります（r=.69）。ただしこのうち相関係数が有意となったものは中国人英語学習者と英語母語話者の相関のみです（[1]の4章）。</p>
<p>では相関分析を行うプログラム examples/correlation_analysis_ceeaus.scala を順に見てみましょう。プログラムの最初の部分（インデックスの指定、スキーマの設定、IReader オブジェクトの作成）は前述の仮説検定で説明した(1)から(3)と同じなので省略します。</p>
<p>プログラムの次の部分(4)はやはり仮説検定のところでも使ったテクニックで、インデックスから各サブコーパスを対象に処理ができるように指定をしています。ここでは指定するサブコーパスに CEECUS を追加しています。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (4)</span>
<span class="kw">val</span> docSetJUS = reader.<span class="fu">subset</span>(<span class="fu">TermFilter</span>(<span class="st">&quot;file&quot;</span>, <span class="st">&quot;ceejus_all.txt&quot;</span>))
<span class="kw">val</span> docSetCUS = reader.<span class="fu">subset</span>(<span class="fu">TermFilter</span>(<span class="st">&quot;file&quot;</span>, <span class="st">&quot;ceecus_all.txt&quot;</span>))
<span class="kw">val</span> docSetNAS = reader.<span class="fu">subset</span>(<span class="fu">TermFilter</span>(<span class="st">&quot;file&quot;</span>, <span class="st">&quot;ceenas_all.txt&quot;</span>))</code></pre>
<p>次にwordsに今回注目する単語のリストを保持します(5)。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (5)</span>
<span class="kw">val</span> words = List(<span class="st">&quot;besides,&quot;</span>, <span class="st">&quot;nevertheless,&quot;</span>, <span class="st">&quot;also,&quot;</span>, <span class="st">&quot;moreover,&quot;</span>, <span class="st">&quot;however,&quot;</span>, <span class="st">&quot;therefore,&quot;</span>, <span class="st">&quot;so,&quot;</span>)</code></pre>
<p>各単語には一般副詞と区別するために後ろにカンマをつけています。このリストを次のWordCountsのcount()関数に指定することでこれらの単語だけに注目してそれぞれのコーパスから単語の出現頻度を取得します。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (6)</span>
<span class="kw">val</span> wcJUS = WordCounts.<span class="fu">count</span>(reader, <span class="st">&quot;body_ws&quot;</span>, words.<span class="fu">toSet</span>, docSetJUS)
<span class="kw">val</span> wcCUS = WordCounts.<span class="fu">count</span>(reader, <span class="st">&quot;body_ws&quot;</span>, words.<span class="fu">toSet</span>, docSetCUS)
<span class="kw">val</span> wcNAS = WordCounts.<span class="fu">count</span>(reader, <span class="st">&quot;body_ws&quot;</span>, words.<span class="fu">toSet</span>, docSetNAS)</code></pre>
<p>ここで注目していただきたいのが対象フィールドにbody_wsを指定している点です。前述の仮説検定ではbody_enフィールドを対象にしていました。</p>
<p>スキーマ設定ファイル examples/schema/ceeaus.confを見ると、body_enはLuceneのStandardAnalyzerを使って作成した(7)を、body_wsはLuceneのWhitespaceTokenizerにLowerCaseFilterを組み合わせて作成した(8)を使って単語分割されています。</p>
<pre class="sourceCode json"><code class="sourceCode json"><span class="er">schema</span> <span class="fu">{</span>
  <span class="er">defAnalyzer</span> <span class="er">{</span>
    <span class="er">class</span> <span class="fu">:</span> <span class="er">org.apache.lucene.analysis.standard.StandardAnalyzer</span>
  <span class="fu">}</span>
  <span class="er">fields　=</span> <span class="ot">[</span>
    <span class="fu">{</span>
      <span class="er">name</span> <span class="fu">:</span> <span class="er">file</span>
      <span class="er">indexed</span> <span class="er">:</span> <span class="kw">true</span>
      <span class="er">stored</span> <span class="er">:</span> <span class="kw">true</span>
    <span class="fu">}</span>
    <span class="fu">{</span>
      <span class="er">name</span> <span class="fu">:</span> <span class="er">type</span>
      <span class="er">indexed</span> <span class="er">:</span> <span class="kw">true</span>
      <span class="er">stored</span> <span class="er">:</span> <span class="kw">true</span>
    <span class="fu">}</span>
    <span class="fu">{</span>
      <span class="er">name</span> <span class="fu">:</span> <span class="er">cat</span>
      <span class="er">indexed</span> <span class="er">:</span> <span class="kw">true</span>
      <span class="er">stored</span> <span class="er">:</span> <span class="kw">true</span>
    <span class="fu">}</span>
    <span class="fu">{</span>
      <span class="er">name</span> <span class="fu">:</span> <span class="er">body_en</span>  <span class="er">//</span> <span class="er">(</span><span class="dv">7</span><span class="er">)</span>
      <span class="er">analyzer</span> <span class="er">:</span> <span class="fu">{</span>
        <span class="er">tokenizer</span> <span class="er">{</span>
          <span class="er">factory</span> <span class="fu">:</span> <span class="er">standard</span>
        <span class="fu">}</span>
        <span class="er">filters</span> <span class="er">=</span> <span class="ot">[</span>
          <span class="fu">{</span>
            <span class="er">factory</span> <span class="fu">:</span> <span class="er">lowercase</span>
          <span class="fu">}</span>
        <span class="ot">]</span>
      <span class="fu">}</span>
      <span class="er">indexed</span> <span class="er">:</span> <span class="kw">true</span>
      <span class="er">stored</span> <span class="er">:</span> <span class="kw">true</span>
      <span class="er">termVector</span> <span class="er">:</span> <span class="kw">true</span>
      <span class="er">positions</span> <span class="er">:</span> <span class="kw">true</span>
    <span class="er">}</span>
    <span class="fu">{</span>
      <span class="er">name</span> <span class="fu">:</span> <span class="er">body_ws</span>  <span class="er">//</span> <span class="er">(</span><span class="dv">8</span><span class="er">)</span>
      <span class="er">analyzer</span> <span class="er">:</span> <span class="fu">{</span>
        <span class="er">tokenizer</span> <span class="er">{</span>
          <span class="er">factory</span> <span class="fu">:</span> <span class="er">whitespace</span>
        <span class="fu">}</span>
        <span class="er">filters</span> <span class="er">=</span> <span class="ot">[</span>
          <span class="fu">{</span>
            <span class="er">factory</span> <span class="fu">:</span> <span class="er">lowercase</span>
          <span class="fu">}</span>
        <span class="ot">]</span>
      <span class="fu">}</span>
      <span class="er">indexed</span> <span class="er">:</span> <span class="kw">true</span>
      <span class="er">stored</span> <span class="er">:</span> <span class="kw">true</span>
      <span class="er">termVector</span> <span class="er">:</span> <span class="kw">true</span>
      <span class="er">positions</span> <span class="er">:</span> <span class="kw">true</span>
    <span class="er">}</span>
    <span class="fu">{</span>
      <span class="er">name</span> <span class="fu">:</span> <span class="er">body_ja</span>
      <span class="er">analyzer</span> <span class="er">:</span> <span class="fu">{</span>
        <span class="er">class</span> <span class="fu">:</span> <span class="er">org.apache.lucene.analysis.ja.JapaneseAnalyzer</span>
      <span class="fu">}</span>
      <span class="er">indexed</span> <span class="er">:</span> <span class="kw">true</span>
      <span class="er">stored</span> <span class="er">:</span> <span class="kw">true</span>
      <span class="er">termVector</span> <span class="er">:</span> <span class="kw">true</span>
      <span class="er">positions</span> <span class="er">:</span> <span class="kw">true</span>
    <span class="fu">}</span>
  <span class="ot">]</span>
<span class="er">}</span></code></pre>
<p>body_wsとbody_enの単語分割の違いを見てみましょう。examples/correlation_analysis_ceeaus.scala プログラムを実行した状態の nlp4l のプロンプトから次のようにしてスキーマのそれぞれのフィールドから Analyzer を取得します。</p>
<pre class="shell"><code>nlp4l&gt; val analyzerWs = schema.getAnalyzer(&quot;body_ws&quot;).get
nlp4l&gt; val analyzerEn = schema.getAnalyzer(&quot;body_en&quot;).get</code></pre>
<p>次にbody_wsフィールドに設定されているanalyzerWsを使って&quot;Also, when we enter college, we are no longer children&quot;という文を単語分割します。</p>
<pre class="shell"><code>nlp4l&gt; analyzerWs.tokens(&quot;Also, when we enter college, we are no longer children&quot;)
res19: org.nlp4l.core.analysis.Tokens = Tokens(Map(startOffset -&gt; 0, type -&gt; word, positionIncrement -&gt; 1, endOffset -&gt; 5, term -&gt; also,, positionLength -&gt; 1, bytes -&gt; [61 6c 73 6f 2c]), Map(startOffset -&gt; 6, type -&gt; word, positionIncrement -&gt; 1, endOffset -&gt; 10, term -&gt; when, positionLength -&gt; 1, bytes -&gt; [77 68 65 6e]), Map(startOffset -&gt; 11, type -&gt; word, positionIncrement -&gt; 1, endOffset -&gt; 13, term -&gt; we, positionLength -&gt; 1, bytes -&gt; [77 65]), Map(startOffset -&gt; 14, type -&gt; word, positionIncrement -&gt; 1, endOffset -&gt; 19, term -&gt; enter, positionLength -&gt; 1, bytes -&gt; [65 6e 74 65 72]), Map(startOffset -&gt; 20, type -&gt; word, positionIncrement -&gt; 1, endOffset -&gt; 28, term -&gt; college,, positionLength -&gt; 1, bytes -&gt; [63 6f 6c 6c 65 67 65 2c]), Map(startOffset -&gt; 29, type -&gt; word, positionIn...</code></pre>
<p>LuceneのAnalyzerは単語分割する際に分割された単語文字列だけでなく、文章内の単語の位置情報や種類など、非常に多くの情報を付加してくれます。そのためこのままだと表示が見にくいので、次のように少し工夫します（Analyzerの使い方は本書で後述します）。</p>
<pre class="shell"><code>nlp4l&gt; analyzerWs.tokens(&quot;Also, when we enter college, we are no longer children.&quot;).map(_.get(&quot;term&quot;).get).foreach(println(_))
also,
when
we
enter
college,
we
are
no
longer
children.</code></pre>
<p>同様に、analyzerEnを使って同じ文章を単語分割してみます。</p>
<pre class="shell"><code>nlp4l&gt; analyzerEn.tokens(&quot;Also, when we enter college, we are no longer children.&quot;).map(_.get(&quot;term&quot;).get).foreach(println(_))
also
when
we
enter
college
we
are
no
longer
children</code></pre>
<p>これらの結果から、analyzerWsを使っているbody_wsフィールドでは単語の後ろについているカンマやピリオドを取り除かないが、analyzerEnを使っているbody_enフィールドでは単語の後ろについているこれらの記号を取り除いていることがわかります。</p>
<p>したがって、&quot;also,&quot;という接続副詞としてのalsoのみを検索したい今回はbody_enではなくbody_wsを処理対象にしています。</p>
<p>ではプログラムに戻りましょう。次の部分(9)では1つめの表である単語出現頻度を表示しています。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (9)</span>
<span class="fu">println</span>(<span class="st">&quot;</span><span class="ch">\n\n\n</span><span class="st">word counts&quot;</span>)
<span class="fu">println</span>(<span class="st">&quot;========================================&quot;</span>)
<span class="fu">println</span>(<span class="st">&quot;</span><span class="ch">\t</span><span class="st">word</span><span class="ch">\t</span><span class="st">CEEJUS</span><span class="ch">\t</span><span class="st">CEECUS</span><span class="ch">\t</span><span class="st">CEENAS&quot;</span>)
words.<span class="fu">foreach</span>{ e =&gt;
  <span class="fu">println</span>(<span class="st">&quot;%10s</span><span class="ch">\t</span><span class="st">%,6d</span><span class="ch">\t</span><span class="st">%,6d</span><span class="ch">\t</span><span class="st">%,6d&quot;</span><span class="fu">.format</span>(e, wcJUS.getOrElse(e, 0), wcCUS.getOrElse(e, 0), wcNAS.getOrElse(e, 0)))
}</code></pre>
<p>プログラムの最後の部分(10)では単相関行列表を表示しています。行列表なので、2重ループのプログラムになっています。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="co">// (10)</span>
<span class="kw">val</span> lj = List( (<span class="st">&quot;CEEJJS&quot;</span>, wcJUS), (<span class="st">&quot;CEECUS&quot;</span>, wcCUS), (<span class="st">&quot;CEENAS&quot;</span>, wcNAS) )
<span class="fu">println</span>(<span class="st">&quot;</span><span class="ch">\n\n\n</span><span class="st">Correlation Coefficient&quot;</span>)
<span class="fu">println</span>(<span class="st">&quot;================================&quot;</span>)
<span class="fu">println</span>(<span class="st">&quot;</span><span class="ch">\t</span><span class="st">CEEJUS</span><span class="ch">\t</span><span class="st">CEECUS</span><span class="ch">\t</span><span class="st">CEENAS&quot;</span>)
lj.<span class="fu">foreach</span>{ ej =&gt;
  <span class="fu">print</span>(<span class="st">&quot;%s&quot;</span><span class="fu">.format</span>(ej._1))
  lj.<span class="fu">foreach</span>{ ei =&gt;
    <span class="fu">print</span>(<span class="st">&quot;</span><span class="ch">\t</span><span class="st">%5.3f&quot;</span><span class="fu">.format</span>(Stats.correlationCoefficient(words.map(ej._2.getOrElse(_, 0.toLong)), words.map(ei._2.getOrElse(_, 0.toLong)))))
  }
  println
}

<span class="co">// (11)</span>
reader.<span class="fu">close</span></code></pre>
<p>相関係数を計算するためにStatsオブジェクトのcorrelationCoefficient()関数を使って求めています。correlationCoefficient()関数には相関を求めたい2つの単語の頻度ベクトルを渡します。</p>
<p>プログラムの最後にreaderをcloseしています(11)。</p>
<p>同じように、ブラウンコーパスを使ってカテゴリ間の相関分析を行ってみます。書籍[3]2.1.3に紹介されている法助動詞（&quot;can&quot;, &quot;could&quot;, &quot;may&quot;, &quot;might&quot;, &quot;must&quot;, &quot;will&quot;）について調べます。プログラム examples/correlation_analysis_brown.scala を実行すると、同様に単語頻度表、単相関行列表が表示されます。</p>
<pre class="shell"><code>nlp4l&gt; :load examples/correlation_analysis_brown.scala

word counts
========================================
word       gov  news  romance  SF
     can   119    94       79  16
   could    38    87      195  49
     may   179    93       11   4
   might    13    38       51  12
    must   102    53       46   8
    will   244   390       49  17


Correlation Coefficient
================================
            gov   news  romance  SF
gov        1.000  0.789 -0.500 -0.385
news       0.789  1.000 -0.127  0.029
romance   -0.500 -0.127  1.000  0.980
SF        -0.385  0.029  0.980  1.000</code></pre>
<p>2つめの表より、governmentとnews および romanceとscience_fiction が互いに強い正の相関があることがわかります。プログラムは前掲のものとほぼ同じなので解説は省略します。</p>
<h1 id="indexBrowser">インデックスブラウザを使う</h1>
<p>NLP4L の対話型シェルには、CUIのLucene インデックスブラウザが付属しています。インデックスブラウザを使うと、Javaプログラムを書かずに、Luceneインデックス中のフィールドや単語の情報を簡単に閲覧、デバッグができます。</p>
<p>ここでは、前の節で作成した、Livedoorニュースコーパスのインデックスの中をブラウズしてみましょう。</p>
<p>なお、インデックスブラウザのコマンド（メソッド）一覧は、nlp4lのプロンプトに&quot;:?&quot;と入力すると見ることができます。</p>
<pre class="shell"><code>nlp4l&gt; :?</code></pre>
<p>また、&quot;:?&quot; のうしろにコマンドをつけると、さらに詳しいヘルプが表示されます。</p>
<pre class="shell"><code>nlp4l&gt; :? open
-- method signature --
def open(idxDir: String): RawReader

-- description --
Open Lucene index in the directory. If an index already opened, that is closed before the new index will be opened.

-- arguments --
idxDir       Lucene index directory

-- return value --
Return : index reader

-- usage --
nlp4l&gt; open(&quot;/tmp/myindex&quot;)</code></pre>
<h2 id="フィールド単語のブラウジング">フィールド、単語のブラウジング</h2>
<p>open() 関数にインデックスディレクトリのパスを渡すことでインデックスをオープンします。</p>
<pre class="shell"><code>nlp4l&gt; open(&quot;/tmp/index-ldcc&quot;)
Index /tmp/index-ldcc was opened.
res4: org.nlp4l.core.RawReader = IndexReader(path=&#39;/tmp/index-ldcc&#39;,closed=false)</code></pre>
<p>close で、現在オープンしているインデックスをクローズします。</p>
<pre class="shell"><code>nlp4l&gt; close
Index /tmp/index-ldcc was closed.</code></pre>
<p>status とタイプすると、現在オープンしているインデックスについて、ドキュメント数やフィールドの情報、各フィールドに含まれるユニークな単語数などの概略が表示されます。</p>
<pre class="shell"><code>nlp4l&gt; open(&quot;/tmp/index-ldcc&quot;)
Index /tmp/index-ldcc was opened.
res4: org.nlp4l.core.RawReader = IndexReader(path=&#39;/tmp/index-ldcc&#39;,closed=false)

nlp4l&gt; status

========================================
Index Path       : /tmp/index-ldcc
Closed           : false
Num of Fields    : 5
Num of Docs      : 7367
Num of Max Docs  : 7367
Has Deletions    : false
========================================
        
Fields Info:
========================================
  # | Name  | Num Terms 
----------------------------------------
  0 | body  |      64543
  1 | url   |       7367
  2 | date  |       6753
  3 | title |      14205
  4 | cat   |          9
========================================</code></pre>
<p>さらに、browseTerms() 関数に、フィールド名を渡すと、nextTerms および prevTerms 関数で、フィールド中に含まれる単語の情報を閲覧できるようになります。</p>
<p>nextTerms(), prevTerms() には、スキップするページ数を渡すことができます。また、nextTerms(1) には nt, prevTerms(1) には pt というショートカット関数が定義されています。</p>
<p>title フィールド中に含まれる単語を閲覧してみましょう。単語は辞書順で並んでおり、各行の冒頭がインデックスされている単語、DF はその単語を含むドキュメント数(document frequency)、Total TF (term frequency) はその単語の出現回数の総計を表します。</p>
<pre class="shell"><code>nlp4l&gt; browseTerms(&quot;title&quot;)
Browse terms for field &#39;title&#39;, page size 20
Type &quot;nextTerms(skip)&quot; or &quot;nt&quot; to browse next terms.
Type &quot;prevTerms(skip)&quot; or &quot;pt&quot; to browse prev terms.
Type &quot;topTerms(n)&quot; to find top n frequent terms.

// nt で次のページ
nlp4l&gt; nt
Indexed terms for field &#39;title&#39;
0 (DF=152, Total TF=176)
000 (DF=13, Total TF=13)
003 (DF=3, Total TF=3)
0048 (DF=1, Total TF=1)
007 (DF=8, Total TF=8)
...

// 何度かページング, または nextTerms(n)でスキップすると、アルファベットから始まる単語が見える
nlp4l&gt; nt
Indexed terms for field &#39;title&#39;
cocorobo (DF=1, Total TF=1)
code (DF=1, Total TF=1)
coin (DF=1, Total TF=1)
collection (DF=3, Total TF=3)
...

// pt で前のページに戻る
nlp4l&gt; pt
Indexed terms for field &#39;title&#39;
chat (DF=1, Total TF=1)
check (DF=2, Total TF=2)
chochokure (DF=1, Total TF=1)
christian (DF=1, Total TF=1)
...</code></pre>
<p>1ページに表示される単語数はデフォルトで20件ですが、browseTerms(&quot;title&quot;,100)のように第２引数でページサイズを指定することもできます。</p>
<p>さらに、browseTermDocs() 関数に、フィールド名と単語を渡すと、nextDocs および prevDocs 関数で、指定されたフィールドにその単語を含むドキュメントの情報を閲覧できるようになります。</p>
<p>nextDocs(), prevDocs() には、スキップするページ数を渡すことができます。また、nextDocs(1) には nd, prevDocs(1) には pd というショートカット関数が定義されています。</p>
<p>title フィールドに &quot;iphone&quot; という単語を含むドキュメントを閲覧してみましょう。ドキュメントはLucene内部のドキュメントID順で並んでおり、idはドキュメントID、freq はドキュメント中に指定の単語（ここでは&quot;iphone&quot;）が出現する頻度を表します。</p>
<p>インデックス時に position や offset (その単語がドキュメント中のどこに出現するかの情報。後述) を保存していると、これらの情報もあわせて表示されます。</p>
<pre class="shell"><code>nlp4l&gt; browseTermDocs(&quot;title&quot;, &quot;iphone&quot;)
Browse docs for term &#39;iphone&#39; in field &#39;title&#39;, page size 20
Type &quot;nextDocs(skip)&quot; or &quot;nd&quot; to browse next terms.
Type &quot;prevDocs(skip)&quot; or &quot;pd&quot; to browse prev terms.

// nd で次のページ
nlp4l&gt; nd
Documents for term &#39;iphone&#39; in field &#39;title&#39;
Doc(id=49, freq=1, positions=List(pos=5))
Doc(id=270, freq=1, positions=List(pos=0))
Doc(id=648, freq=1, positions=List(pos=0))
Doc(id=653, freq=1, positions=List(pos=2))
Doc(id=778, freq=1, positions=List(pos=2))
Doc(id=780, freq=2, positions=List(pos=0, pos=15))
...

// pd で前のページに戻る
nlp4l&gt; pd
Documents for term &#39;iphone&#39; in field &#39;title&#39;
Doc(id=1173, freq=1, positions=List(pos=1))
Doc(id=1176, freq=1, positions=List(pos=0))
Doc(id=1180, freq=1, positions=List(pos=2))
Doc(id=1195, freq=1, positions=List(pos=5))
Doc(id=1200, freq=1, positions=List(pos=11))
Doc(id=1203, freq=1, positions=List(pos=5))
...</code></pre>
<p>1ページに表示されるドキュメント数はデフォルトで20件ですが、browseTermDocs(&quot;title&quot;,&quot;iphone&quot;,100)のように第３引数でページサイズを指定することもできます。</p>
<h2 id="ドキュメントの閲覧">ドキュメントの閲覧</h2>
<p>より詳しくドキュメントの内容を見たい場合は、showDoc() 関数にドキュメントIDを渡すことでフィールド値を表示させることができます。</p>
<p>前章の browseTermDocs / nd / pd 関数で取得したドキュメントID (ここでは id=1195) に対応するドキュメント内容を表示してみましょう。&quot;iPhone&quot;という単語が、title フィールドに出現することが確認できます。</p>
<pre class="shell"><code>nlp4l&gt; showDoc(1195)
Doc #1195
(Field) cat: [it-life-hack]
(Field) url: [http://news.livedoor.com/article/detail/6608703/]
(Field) title: [GoogleドライブのファイルをiPhoneからダイレクトに編集する【知っ得！虎の巻】]
...</code></pre>
<h2 id="position-offsets">Position / Offsets</h2>
<p>Position / Offsets はインデックスに付加的に保存できる情報で、各単語のドキュメント中の出現箇所を示します。</p>
<p>たとえば、以下の例のドキュメントID=199のドキュメントに着目すると、</p>
<ul>
<li>body フィールドに &quot;北海道&quot; という単語が2回出現する</li>
<li>最初の出現は 126 単語め(position=126)で、詳細な文字位置は 247-250</li>
<li>2回めの出現は 129 単語め(position=129)で、詳細な文字位置は 255-258</li>
</ul>
<p>ということがわかります。</p>
<pre class="shell"><code>nlp4l&gt; browseTermDocs(&quot;body&quot;,&quot;北海道&quot;)
Browse docs for term body in field 北海道, page size 20
Type &quot;nextDocs(skip)&quot; or &quot;nd&quot; to browse next terms.
Type &quot;prevDocs(skip)&quot; or &quot;pd&quot; to browse prev terms.

nlp4l&gt; nd
Documents for term &#39;北海道&#39; in field &#39;body&#39;
Doc(id=148, freq=1, positions=List((pos=491,offset={997-1000})))
Doc(id=199, freq=2, positions=List((pos=126,offset={247-250}), (pos=129,offset={255-258})))
...</code></pre>
<h2 id="出現頻度の高い-top-n-単語の抽出">出現頻度の高い Top N 単語の抽出</h2>
<p>browseTerms() でフィールドを指定した後、topTerms() 関数でそのフィールドでドキュメント出現頻度(DF)の高い上位N件の単語とその頻度を取得できます。</p>
<pre class="shell"><code>nlp4l&gt; browseTerms(&quot;title&quot;)
Browse terms for field title, page size 20
Type &quot;nextTerms(skip)&quot; or &quot;nt&quot; to browse next terms.
Type &quot;prevTerms(skip)&quot; or &quot;pt&quot; to browse prev terms.
Type &quot;topTerms(n)&quot; to find top n frequent terms.

nlp4l&gt; topTerms(10)
Top 10 frequent terms for field title
  1: 話題 (DF=587, Total TF=607)
  2: sports (DF=493, Total TF=493)
  3: watch (DF=493, Total TF=493)
  4: 映画 (DF=373, Total TF=402)
  5: 女 (DF=319, Total TF=356)
  6: 1 (DF=318, Total TF=342)
  7: android (DF=307, Total TF=322)
  8: 女子 (DF=301, Total TF=318)
  9: 3 (DF=299, Total TF=323)
 10: アプリ (DF=291, Total TF=337)</code></pre>
<h1 id="dearSolrUsers">Solrユーザの皆様</h1>
<h1 id="dearESUsers">Elasticsearchユーザの皆様</h1>
<h1 id="useWithMahout">Mahoutと連携する</h1>
<h1 id="useWithSpark">Sparkと連携する</h1>
<p>この節では、事前に Spark 1.3.0 以上がインストールされていることが必要です。(1.3.0 以前の Spark をお使いの場合は、適宜読み替えてください。)</p>
<h2 id="mllibと連携する">MLLibと連携する</h2>
<p>NLP4L でコーパスの特徴量を抽出し、 Spark MLlib への入力として与えることができます。</p>
<h3 id="useWithSpark_svm">サポートベクトルマシンで文書分類</h3>
<p>Spark MLlib にはいくつかの分類器が提供されていますが、ここではサポートベクトルマシン（SVM）を用いてlivedoorニュースコーパス（ldcc）を文書分類する方法を紹介します。</p>
<p>まだ実行していない場合は examples/index_ldcc.scala を実行して ldcc のコーパスをLuceneインデックスに用意します。次に examples/extract_ldcc.scala を使ってLuceneインデックスから2つのカテゴリー&quot;dokujo-tsushin&quot; と &quot;sports-watch&quot;を持つ記事だけを抽出し、/tmp/index-ldcc-partという名前の別のLuceneインデックスを作成します。</p>
<pre class="shell"><code>nlp4l&gt; :load examples/extract_ldcc.scala</code></pre>
<p>次にコマンドラインプログラム LabeledPointAdapter を次のように実行して、/tmp/index-ldcc-part から2クラスの全文書のベクトルデータを出力します。</p>
<pre class="shell"><code>$ java -Dfile.encoding=UTF-8 -cp &quot;target/pack/lib/*&quot; org.nlp4l.spark.mllib.LabeledPointAdapter -s examples/schema/ldcc.conf -f body -l cat /tmp/index-ldcc-part</code></pre>
<p>ここで -s オプションにはスキーマ定義ファイル名を、-f オプションには特徴ベクトル抽出対象のフィールド名を、-l にはラベルが記録されているフィールド名を指定します。実行結果は labeled-point-out/ ディレクトリ以下に出力されます。labeled-point-out/data.txt ファイルは Spark MLlib への入力とすることができる libsvm 形式のファイルになっています。1カラム目が数値ラベル、2カラム目以降が特徴ベクトルです。数値ラベルと実際のラベル名の対応関係は labeled-point-out/label.txt ファイルに出力されます。</p>
<pre class="shell"><code>$ cat labeled-point-out/label.txt
dokujo-tsushin  0
sports-watch    1</code></pre>
<p>ではSpark MLlib のSVMを実行してみましょう。spark-shell を起動して次のプログラムを実行します。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">import</span> org.<span class="fu">apache</span>.<span class="fu">spark</span>.<span class="fu">SparkContext</span>
<span class="kw">import</span> org.<span class="fu">apache</span>.<span class="fu">spark</span>.<span class="fu">mllib</span>.<span class="fu">classification</span>.{SVMModel, SVMWithSGD}
<span class="kw">import</span> org.<span class="fu">apache</span>.<span class="fu">spark</span>.<span class="fu">mllib</span>.<span class="fu">evaluation</span>.<span class="fu">BinaryClassificationMetrics</span>
<span class="kw">import</span> org.<span class="fu">apache</span>.<span class="fu">spark</span>.<span class="fu">mllib</span>.<span class="fu">regression</span>.<span class="fu">LabeledPoint</span>
<span class="kw">import</span> org.<span class="fu">apache</span>.<span class="fu">spark</span>.<span class="fu">mllib</span>.<span class="fu">linalg</span>.<span class="fu">Vectors</span>
<span class="kw">import</span> org.<span class="fu">apache</span>.<span class="fu">spark</span>.<span class="fu">mllib</span>.<span class="fu">util</span>.<span class="fu">MLUtils</span>

<span class="co">// Load training data in LIBSVM format.</span>
<span class="kw">val</span> data = MLUtils.<span class="fu">loadLibSVMFile</span>(sc, <span class="st">&quot;labeled-point-out/data.txt&quot;</span>)

<span class="co">// Split data into training (70%) and test (30%).</span>
<span class="kw">val</span> splits = data.<span class="fu">randomSplit</span>(Array(<span class="fl">0.7</span>, <span class="fl">0.3</span>), seed = 11L)
<span class="kw">val</span> training = <span class="fu">splits</span>(<span class="dv">0</span>).<span class="fu">cache</span>()
<span class="kw">val</span> test = <span class="fu">splits</span>(<span class="dv">1</span>)

<span class="co">// Run training algorithm to build the model</span>
<span class="kw">val</span> numIterations = <span class="dv">100</span>
<span class="kw">val</span> model = SVMWithSGD.<span class="fu">train</span>(training, numIterations)

<span class="co">// Clear the default threshold.</span>
model.<span class="fu">clearThreshold</span>()

<span class="co">// Compute raw scores on the test set.</span>
<span class="kw">val</span> scoreAndLabels = test.<span class="fu">map</span> { point =&gt;
  <span class="kw">val</span> score = model.<span class="fu">predict</span>(point.<span class="fu">features</span>)
  (score, point.<span class="fu">label</span>)
}

<span class="co">// Get evaluation metrics.</span>
<span class="kw">val</span> metrics = <span class="kw">new</span> <span class="fu">BinaryClassificationMetrics</span>(scoreAndLabels)
<span class="kw">val</span> auROC = metrics.<span class="fu">areaUnderROC</span>()

<span class="fu">println</span>(<span class="st">&quot;Area under ROC = &quot;</span> + auROC)</code></pre>
<p>実行結果が次のように表示されます。Area under ROCが0.9989017178259646と表示されました。</p>
<pre class="sourceCode scala"><code class="sourceCode scala">Area under ROC = <span class="fl">0.9989017178259646</span></code></pre>
<h3 id="クラスタリング">クラスタリング</h3>
<p>Spark MLlib にはいくつかのクラスタリングアルゴリズム(k-means, Gaussian mixture 等)が実装されていますが、ここでは LDA (Latent Dirichlet allocation) を利用する例を紹介します。なお、Spark MLlib に実装されているクラスタリングアルゴリズムの詳細は <a href="http://spark.apache.org/docs/1.3.0/mllib-clustering.html">公式リファレンス (v1.3.0)</a> を参照してください。</p>
<p>注: LDA は Spark 1.3.0 から導入されました。</p>
<p>コマンドラインから、プログラム VectorsAdapter を実行します。(事前に、examples/index_ldcc.scalaを実行して livedoor ニュースコーパスをインデックスしておいてください。)</p>
<pre><code>$ java -Dfile.encoding=UTF-8 -cp &quot;target/pack/lib/*&quot; org.nlp4l.spark.mllib.VectorsAdapter -s examples/schema/ldcc.conf -f body --idfmode n --type int /tmp/index-ldcc</code></pre>
<p>実行すると、vectors-out ディレクトリ以下に2つのファイルが生成されます。</p>
<ul>
<li>data.txt // データファイル(カンマ区切りCSV形式)。一行目はヘッダ(単語ID)。各行の1カラム目は文書ID。</li>
<li>words.txt // 単語IDと単語の一覧</li>
</ul>
<p>特徴量を抽出したら、Spark でLDAを実行します。spark-shell を起動し、以下のように入力してください。</p>
<pre class="shell"><code>$ spark-shell

scala&gt; import org.apache.spark.mllib.clustering.LDA
scala&gt; import org.apache.spark.mllib.linalg.Vectors
// data.txt を入力として与える
scala&gt; val data = sc.textFile(&quot;/path/to/vectors-out/data.txt&quot;)
scala&gt; val parsedData = data.map(s =&gt; Vectors.dense(s.trim.split(&#39; &#39;).map(_.toDouble)))
scala&gt; val corpus = parsedData.zipWithIndex.map(_.swap).cache()
// K=5 を指定してモデルを作成する
scala&gt; val ldaModel = new LDA().setK(5).run(corpus)

// 推定されたトピックを取得する. トピックは各単語の出現確率分布として表現される.
scala&gt; val topics = ldaModel.topicsMatrix
6.582992067532604     0.12712473287343715   2.0749605231892994    ... (5 total)
1.7458039064383513    0.00886714658883468   4.228671695274331     ...
0.993056435220057     4.64132780991838      0.18921245384121668   ...
...

// 各ドキュメントがどのトピックに属するかの確率分布を取得する.
scala&gt; val topics = ldaModel.topicDistributions
scala&gt; topics.take(5).foreach(println)
(384,[0.13084037186524378,0.02145904901484863,0.30073967633170434,0.18175275728283377,0.36520814550536945])
(204,[0.5601036760461913,0.04276689792374281,0.17626863743620377,0.06992184061352519,0.15093894798033694])
(140,[0.01548241660044312,0.8975654153324738,0.013671563672420709,0.061526631681883964,0.011753972712778454])
(466,[0.052798328682649866,0.04602366817727088,0.7138181945792464,0.03541828992076265,0.1519415186400702])
(160,[0.20118704750574637,0.12811775189441738,0.23204896620959134,0.1428791353110324,0.29576709907921245])</code></pre>
<p>詳しい Spark MLlib の使い方は Spark の API ドキュメントを参照してください。</p>
<h1 id="useLucene">Luceneを使う</h1>
<h2 id="nlp4lが提供するluceneの機能">NLP4Lが提供するLuceneの機能</h2>
<h2 id="analyzer">Analyzer</h2>
<h2 id="既存インデックスを検索する">既存インデックスを検索する</h2>
<h2 id="簡単ファセット">簡単ファセット</h2>
<h2 id="term-vector-から-tfidf-値を求める">Term Vector から TF/IDF 値を求める</h2>
<h2 id="文書類似度を計算する">文書類似度を計算する</h2>
<h2 id="more-like-this">More Like This</h2>
<h2 id="独自インデックスの作成">独自インデックスの作成</h2>
<h3 id="schemaの定義">Schemaの定義</h3>
<h4 id="数値型">数値型</h4>
<p>NLP4LをNLPツールとして使う場合、IntやLongなどの数値型のフィールドを意識する必要はないかもしれません。しかし、Luceneはこれらの型をサポートしており、それぞれの型に適した形で値をインデックスに保存しています。ここでは簡単にこれらの型を扱う方法を説明します。</p>
<h3 id="文書の登録">文書の登録</h3>
<h2 id="fst-を単語辞書として使う">FST を単語辞書として使う</h2>
<p>NLP4LはLuceneのFSTを簡単な単語辞書として使えるようにシンプルにラップしたSimpleFSTを提供しています。FSTを単語辞書として使うと、対象文字列の最左部分文字列（左端から始まるすべての接頭辞）が1回の走査で探索できるため、Luceneプロジェクトでは日本語形態素解析器やシノニム検索で利用されています。</p>
<p>SimpleFSTを利用したサンプルプログラム examples/use_fst.scala を以下に示します。</p>
<pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">import</span> org.<span class="fu">nlp4l</span>.<span class="fu">core</span>.<span class="fu">_</span>

<span class="co">// (1)</span>
<span class="kw">val</span> index = <span class="st">&quot;/tmp/index-brown&quot;</span>
<span class="kw">val</span> schema = SchemaLoader.<span class="fu">load</span>(<span class="st">&quot;examples/schema/brown.conf&quot;</span>)
<span class="kw">val</span> reader = <span class="fu">IReader</span>(index, schema)

<span class="co">// (2)</span>
<span class="kw">val</span> fst = <span class="fu">SimpleFST</span>()

<span class="co">// (3)</span>
reader.<span class="fu">field</span>(<span class="st">&quot;body_pos&quot;</span>).<span class="fu">get</span>.<span class="fu">terms</span>.<span class="fu">foreach</span> { term =&gt;
  fst.<span class="fu">addEntry</span>(term.<span class="fu">text</span>, term.<span class="fu">totalTermFreq</span>)
}

<span class="co">// (4)</span>
fst.<span class="fu">finish</span>

<span class="co">// (5)</span>
<span class="kw">val</span> STR = <span class="st">&quot;iaminnewyork&quot;</span>

<span class="co">// (6)</span>
<span class="kw">for</span>(pos &lt;- <span class="dv">0</span> to STR.<span class="fu">length</span> - <span class="dv">1</span>){
  fst.<span class="fu">leftMostSubstring</span>(STR, pos).<span class="fu">foreach</span> { e =&gt;
    <span class="fu">print</span>(<span class="st">&quot;%s&quot;</span><span class="fu">.format</span>(<span class="st">&quot;             &quot;</span>.substring(0, pos)))
    <span class="fu">println</span>(<span class="st">&quot;%s =&gt; %d&quot;</span><span class="fu">.format</span>(STR.substring(pos, e._1), e._2))
  }
}</code></pre>
<p>このプログラムは前述のブラウンコーパスを登録したLuceneインデックスを参照し、ブラウンコーパスで使用されているすべての単語をFSTに登録します。そのため、あらかじめ examples/index_brown.scala を実行しておく必要があります。</p>
<p>(1)でブラウンコーパスが登録されているインデックスをオープンします。(2)でSimpleFSTのインスタンスを取得しています。(3)で、(1)でオープンしたブラウンコーパスのインデックスのbody_posフィールドから単語一覧を取得しながら、SimpleFSTのaddEntry()を使って単語とその出現頻度をSimpleFSTに登録しています。addEntry()の呼び出しは、ソート済みの文字列リストを使って順に呼び出さなければならないことに注意してください。このプログラムではソートらしきことはしていないように見えますが、Luceneインデックスには単語がソート済みの状態で登録されているため、明示的なソートは不要になっています。登録が終了したら、(4)でfinish()を呼んでいます。</p>
<p>(5)以降は、作成したSimpleFSTの単語辞書を使って文字列STR（&quot;I am in New York&quot;という文を小文字にして詰めた文字列です）にどんな単語が含まれているか、辞書引きをしながらパースしています。(6)のforループは文字列STRの走査開始位置を左から1文字ずつずらしています。最左部分文字列を走査するにはSimpleFSTのleftMostSubstring()を使います。第1引数には走査する文字列を、第2引数には捜査開始位置を渡します。戻り値は(Int,Long)のタプルのSeqで返ってきます。タプルは順に部分文字列の終了位置、単語登録時に指定されたLong値（ここでは当該単語の出現頻度）です。ためしに&quot;welcome&quot;という文字列を捜査開始位置0で辞書引きしてみると次のようになります。</p>
<pre class="shell"><code>nlp4l&gt; fst.leftMostSubstring(&quot;welcome&quot;, 0)
res7: Seq[(Int, Long)] = List((1,5), (2,2656), (7,2705))</code></pre>
<p>タプルが3つ返ってきました。最初のタプル(1,5)は終了位置が1の単語（つまり単語&quot;w&quot;）が5回、2番目のタプル(2,2656)は終了位置が2の単語（つまり単語&quot;we&quot;）が2656回、3番目のタプル(7,2705)は終了位置が7の単語（つまり単語&quot;welcome&quot;）が2705回ブラウンコーパスに出現したことを意味しています。また、終了位置がこれら以外の単語（&quot;wel&quot;,&quot;welc&quot;,&quot;welco&quot;,&quot;welcom&quot;）は作成した単語辞書にないこともわかります。</p>
<p>さて、最初のサンプルプログラムの実行結果は次のようになります。なんとなくですが、&quot;I am in New York&quot; という単語で区切ったときにそれぞれの単語が頻出していることが読み取れると思います。</p>
<pre class="shell"><code>nlp4l&gt; :load examples/use_fst.scala
i =&gt; 5164
 a =&gt; 23195
 am =&gt; 23431
  m =&gt; 16
  mi =&gt; 18
  min =&gt; 22
   i =&gt; 5164
   in =&gt; 26500
   inn =&gt; 26508
    n =&gt; 38
     n =&gt; 38
     ne =&gt; 43
     new =&gt; 1677
      e =&gt; 40
       w =&gt; 5
        y =&gt; 5
        york =&gt; 306
         o =&gt; 26
         or =&gt; 4231
          r =&gt; 38
           k =&gt; 10</code></pre>
<p>SimpleFSTにはleftMostSubstring()以外にも、メモリ上に作成した単語辞書をディスクに保存するsave()や、保存した単語辞書を読み込むためのload()という関数もありますので、いろいろ活用してください。</p>
<h1 id="develop">NLP4Lプログラムを開発して実行する</h1>
<h2 id="replから実行する">REPLから実行する</h2>
<h2 id="コンパイルして実行する">コンパイルして実行する</h2>
<h1 id="tm">帰属</h1>
<p>Apache Lucene, Solr and their logos are trademarks of the Apache Software Foundation.</p>
<p>Elasticsearch is a trademark of Elasticsearch BV, registered in the U.S. and in other countries.</p>
</body>
</html>
